{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ™ï¸ Week2: Speech-to-Text ê°•ì˜ ë…¸íŠ¸ (Colab)\n",
        "\n",
        "> **ì£¼ì˜:** Colab ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘í•˜ë©´ `OPENAI_API_KEY`ë¥¼ ë‹¤ì‹œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "## ì§„í–‰ ìˆœì„œ\n",
        "1. Quickstart: í™˜ê²½ ì¤€ë¹„ì™€ Audio API ê°œìš”\n",
        "2. Transcriptions ì—”ë“œí¬ì¸íŠ¸ ê¸°ë³¸ íë¦„\n",
        "3. ê³ ê¸‰ ì˜µì…˜: ì‘ë‹µ í¬ë§· Â· í”„ë¡¬í”„íŠ¸ Â· ë¡œê·¸í™•ë¥ \n",
        "4. ë°œí™”ì êµ¬ë¶„(Diarization) ì›Œí¬í”Œë¡œìš°\n",
        "5. Translations ì—”ë“œí¬ì¸íŠ¸ë¡œ ì˜ì–´ ë²ˆì—­\n",
        "6. íƒ€ì„ìŠ¤íƒ¬í”„/ìŠ¤íŠ¸ë¦¬ë°/ê¸´ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì•„ì´ë””ì–´\n",
        "7. í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸/í›„ì²˜ë¦¬ ì „ëµ\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Quickstart: í™˜ê²½ ì¤€ë¹„ì™€ Audio API ê°œìš”\n",
        "Speech-to-Text(STT)ëŠ” Audio APIì˜ `transcriptions`ì™€ `translations` ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•´ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. íŒŒì¼ ì—…ë¡œë“œëŠ” 25MB ì´í•˜ë¡œ ì œí•œë˜ë©° `mp3`, `wav`, `webm` ë“±ì„ ì§€ì›í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from pathlib import Path\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not OPENAI_API_KEY:\n",
        "    OPENAI_API_KEY = getpass(\"Enter your OPENAI_API_KEY: \")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "print(\"âœ… API í‚¤ ì„¤ì • ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI, AsyncOpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "async_client = AsyncOpenAI()\n",
        "print(\"âœ… í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Transcriptions ì—”ë“œí¬ì¸íŠ¸ ê¸°ë³¸ íë¦„\n",
        "- `client.audio.transcriptions.create`ë¡œ ì˜¤ë””ì˜¤ íŒŒì¼ê³¼ ëª¨ë¸ì„ ì§€ì •í•´ í…ìŠ¤íŠ¸ë¥¼ ì–»ìŠµë‹ˆë‹¤.\n",
        "- ì¶”ì²œ ëª¨ë¸: `gpt-4o-transcribe`, `gpt-4o-mini-transcribe`\n",
        "- Whisper ê¸°ë°˜ ê¸°ë³¸ ëª¨ë¸: `whisper-1`\n",
        "- ì‘ë‹µì€ ê¸°ë³¸ì ìœ¼ë¡œ JSON(`response.text`)ì´ë©°, `response_format=\"text\"`ë¡œ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transcribe_basic(audio_path):\n",
        "    path = Path(audio_path)\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(path)\n",
        "\n",
        "    with path.open(\"rb\") as audio_file:\n",
        "        transcription = client.audio.transcriptions.create(\n",
        "            model=\"gpt-4o-transcribe\",\n",
        "            file=audio_file,\n",
        "        )\n",
        "    print(\"ì „ì‚¬ ê²°ê³¼ ì˜ˆì‹œ:\\n\", transcription.text[:200])\n",
        "\n",
        "# transcribe_basic(\"data/sample_lecture.mp3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) ê³ ê¸‰ ì˜µì…˜: ì‘ë‹µ í¬ë§· Â· í”„ë¡¬í”„íŠ¸ Â· ë¡œê·¸í™•ë¥ \n",
        "- `response_format`: `json`, `text`, `verbose_json`, `diarized_json` ë“± ì„ íƒ ê°€ëŠ¥\n",
        "- `prompt`: ëª¨ë¸ì´ ì˜ëª» ì¸ì‹í•˜ëŠ” ìš©ì–´/ê³ ìœ ëª…ì‚¬ë¥¼ ì•ˆë‚´í•´ ì •í™•ë„ í–¥ìƒ\n",
        "- `include=[\"logprobs\"]`: í† í°ë³„ í™•ë¥ ì„ í™•ì¸í•´ ì‹ ë¢°ë„ë¥¼ ë¶„ì„\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transcribe_with_options(audio_path, *, prompt=None, response_format=\"json\"):\n",
        "    path = Path(audio_path)\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(path)\n",
        "\n",
        "    kwargs = {\n",
        "        \"model\": \"gpt-4o-transcribe\",\n",
        "        \"response_format\": response_format,\n",
        "    }\n",
        "    if prompt:\n",
        "        kwargs[\"prompt\"] = prompt\n",
        "    if response_format == \"json\":\n",
        "        kwargs[\"include\"] = [\"logprobs\"]\n",
        "\n",
        "    with path.open(\"rb\") as audio_file:\n",
        "        transcription = client.audio.transcriptions.create(\n",
        "            file=audio_file,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    print(\"í…ìŠ¤íŠ¸ ì˜ˆì‹œ:\", transcription.text[:120])\n",
        "    if response_format == \"json\" and getattr(transcription, \"logprobs\", None):\n",
        "        print(\"ì²« í† í° ë¡œê·¸í™•ë¥ :\", transcription.logprobs[0])\n",
        "\n",
        "# transcribe_with_options(\"data/sample_lecture.mp3\", prompt=\"This is a talk about GPT-4o and Sora.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) ë°œí™”ì êµ¬ë¶„(Diarization) ì›Œí¬í”Œë¡œìš°\n",
        "`gpt-4o-transcribe-diarize` ëª¨ë¸ì€ `response_format=\"diarized_json\"`ì„ í†µí•´ `speaker`, `start`, `end` ì •ë³´ë¥¼ í¬í•¨í•œ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤. 30ì´ˆ ì´ìƒ íŒŒì¼ì€ `chunking_strategy=\"auto\"` ì§€ì •ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def diarize_demo(audio_path):\n",
        "    path = Path(audio_path)\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(path)\n",
        "\n",
        "    with path.open(\"rb\") as audio_file:\n",
        "        transcript = client.audio.transcriptions.create(\n",
        "            model=\"gpt-4o-transcribe-diarize\",\n",
        "            file=audio_file,\n",
        "            response_format=\"diarized_json\",\n",
        "            chunking_strategy=\"auto\",\n",
        "        )\n",
        "\n",
        "    for segment in transcript.segments[:5]:\n",
        "        print(f\"{segment.speaker}: {segment.text} ({segment.start:.1f}s ~ {segment.end:.1f}s)\")\n",
        "\n",
        "# diarize_demo(\"data/meeting.wav\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **ê°•ì˜ í¬ì¸íŠ¸**\n",
        "> - `known_speaker_names[]`, `known_speaker_references[]`ë¥¼ ì´ìš©í•´ ì‚¬ì „ì— ë“±ë¡í•œ ìŒì„±ì„ íŠ¹ì • ë°œí™”ìì™€ ë§¤ì¹­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "> - ìŠ¤íŠ¸ë¦¬ë°(`stream=True`)ì„ ì„¤ì •í•˜ë©´ ì„¸ê·¸ë¨¼íŠ¸ ì™„ë£Œ ì‹œì ë§ˆë‹¤ `transcript.text.segment` ì´ë²¤íŠ¸ê°€ ë„ì°©í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Translations ì—”ë“œí¬ì¸íŠ¸ë¡œ ì˜ì–´ ë²ˆì—­\n",
        "`whisper-1` ëª¨ë¸ ì „ìš© ê¸°ëŠ¥ìœ¼ë¡œ, ë¹„ì˜ì–´ ìŒì„±ì„ ì˜ì–´ í…ìŠ¤íŠ¸ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤. ì›ë¬¸ ì–¸ì–´ê°€ ë¬´ì—‡ì´ë“  ì¶œë ¥ì€ ì˜ì–´ì…ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate_demo(audio_path):\n",
        "    path = Path(audio_path)\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(path)\n",
        "\n",
        "    with path.open(\"rb\") as audio_file:\n",
        "        translation = client.audio.translations.create(\n",
        "            model=\"whisper-1\",\n",
        "            file=audio_file,\n",
        "        )\n",
        "\n",
        "    print(\"ì˜ì–´ ë²ˆì—­ ê²°ê³¼:\", translation.text[:200])\n",
        "\n",
        "# translate_demo(\"data/german.mp3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) íƒ€ì„ìŠ¤íƒ¬í”„/ìŠ¤íŠ¸ë¦¬ë°/ê¸´ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì•„ì´ë””ì–´\n",
        "- `timestamp_granularities=[\"word\"]`: `whisper-1`ì—ì„œ ë‹¨ì–´ ë‹¨ìœ„ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ (`response_format=\"verbose_json\"`)\n",
        "- `stream=True`: ì™„ë£Œëœ ì˜¤ë””ì˜¤ íŒŒì¼ë„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µìœ¼ë¡œ ë°›ì•„ ë¹ ë¥´ê²Œ í…ìŠ¤íŠ¸ë¥¼ í‘œì‹œ\n",
        "- Realtime API: WebSocket(`wss://api.openai.com/v1/realtime?intent=transcription`)ìœ¼ë¡œ ì‹¤ì‹œê°„ ìŒì„± ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ê°€ëŠ¥\n",
        "- 25MB ì´ˆê³¼ íŒŒì¼: PyDub ë“±ìœ¼ë¡œ ë¶„í• í•˜ê³ , ë¬¸ì¥ ê²½ê³„ì—ì„œ ë‚˜ëˆ„ë©° ì´ì „ ì„¸ê·¸ë¨¼íŠ¸ í…ìŠ¤íŠ¸ë¥¼ promptë¡œ ì „ë‹¬í•˜ë©´ í’ˆì§ˆ ìœ ì§€ì— ë„ì›€\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transcribe_with_timestamps(audio_path):\n",
        "    path = Path(audio_path)\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(path)\n",
        "\n",
        "    with path.open(\"rb\") as audio_file:\n",
        "        transcription = client.audio.transcriptions.create(\n",
        "            model=\"whisper-1\",\n",
        "            file=audio_file,\n",
        "            response_format=\"verbose_json\",\n",
        "            timestamp_granularities=[\"word\"],\n",
        "        )\n",
        "\n",
        "    for word_info in transcription.words[:5]:\n",
        "        print(word_info)\n",
        "\n",
        "# transcribe_with_timestamps(\"data/sample_lecture.mp3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸/í›„ì²˜ë¦¬ ì „ëµ\n",
        "### Whisper Prompt í™œìš©\n",
        "- `prompt` íŒŒë¼ë¯¸í„°ì— ê³ ìœ ëª…ì‚¬/ì•½ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì œê³µí•´ ì¸ì‹ ì •í™•ë„ë¥¼ ë³´ì •\n",
        "- 224 í† í° ì œí•œìœ¼ë¡œ ëª©ë¡ì´ ê¸¸ë©´ ì ì ˆíˆ ìš”ì•½ í•„ìš”\n",
        "- ì´ì „ ì„¸ê·¸ë¨¼íŠ¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ promptë¡œ ì „ë‹¬í•´ ë§¥ë½ ìœ ì§€ ê°€ëŠ¥\n",
        "\n",
        "### GPT-4 ê¸°ë°˜ í›„ì²˜ë¦¬\n",
        "- ì „ì‚¬ ê²°ê³¼ë¥¼ GPT-4ì— ì „ë‹¬í•´ ì˜¤íƒ€ ìˆ˜ì •, ë§ì¶¤ë²• ë³´ì •, ìŠ¤íƒ€ì¼ í†µì¼ ê°€ëŠ¥\n",
        "- ê¸´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë‹¤ë£° ìˆ˜ ìˆì–´ promptë³´ë‹¤ í™•ì¥ì„± ë†’ìŒ\n",
        "\n",
        "> **ì‹¤ìŠµ ì—°ê²°**: í•™ìƒë“¤ì—ê²Œ prompt íŠœë‹ê³¼ í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì§ì ‘ ì‹¤í—˜í•˜ê²Œ í•˜ê³ , í’ˆì§ˆ í–¥ìƒ ì „/í›„ ë¹„êµë¥¼ ê³¼ì œë¡œ ì œì‹œí•´ ë³´ì„¸ìš”.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ğŸ’¡ **ê°•ì˜ ë§ˆë¬´ë¦¬**\n",
        "> - ì‹¤ìŠµ ë…¸íŠ¸(`04-STT-Practice.ipynb`)ì™€ ì—°ê³„í•´ ê¸°ë³¸ ì „ì‚¬ â†’ ê³ ê¸‰ ì˜µì…˜ â†’ ë°œí™”ì êµ¬ë¶„ â†’ ë²ˆì—­ â†’ íƒ€ì„ìŠ¤íƒ¬í”„ê¹Œì§€ ìˆœì°¨ì ìœ¼ë¡œ ì—°ìŠµí•˜ë„ë¡ ì•ˆë‚´í•˜ì„¸ìš”.\n",
        "> - ìŠ¤íŠ¸ë¦¬ë°/Realtime APIëŠ” ë°ëª¨ ì‹œê°„ì´ ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì‚¬ì „ ë…¹í™”í•œ ë°ëª¨ í™”ë©´ì´ë‚˜ ë¡œê·¸ë¥¼ ì¤€ë¹„í•´ ì„¤ëª…ì„ ë³´ì™„í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
