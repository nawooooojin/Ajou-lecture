{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ¬ Week2: Sora ë¹„ë””ì˜¤ ìƒì„± ê°•ì˜ ë…¸íŠ¸ (Colab)\n",
        "\n",
        "> **ì£¼ì˜:** Colab ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘í•˜ë©´ `OPENAI_API_KEY` í™˜ê²½ ë³€ìˆ˜ë¥¼ ë‹¤ì‹œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "## ì§„í–‰ ìˆœì„œ\n",
        "1. Quickstart: í™˜ê²½ ì¤€ë¹„ ë° API ê°œìš”\n",
        "2. Sora ëª¨ë¸ ë¼ì¸ì—… (sora-2 vs sora-2-pro)\n",
        "3. ë¹„ë””ì˜¤ ìƒì„± ìš”ì²­ íë¦„(ë¹„ë™ê¸° ë Œë”)\n",
        "4. ìƒíƒœ ëª¨ë‹ˆí„°ë§: í´ë§ vs ì›¹í›…\n",
        "5. ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ë° ë³´ì¡° ì—ì…‹ í™œìš©\n",
        "6. ì…ë ¥ ì´ë¯¸ì§€/ë¦¬ë¯¹ìŠ¤ ë“± ê³ ê¸‰ ê¸°ëŠ¥\n",
        "7. ë¼ì´ë¸ŒëŸ¬ë¦¬ ê´€ë¦¬ & ì½˜í…ì¸  ì •ì±…\n",
        "8. ë¹„ìš©Â·ì§€ì—°Â·í•œê³„ì™€ ì‹¤ìŠµ ê³¼ì œ\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Quickstart: í™˜ê²½ ì¤€ë¹„ ë° API ê°œìš”\n",
        "Sora ë¹„ë””ì˜¤ APIëŠ” ë¯¸ë¦¬ë³´ê¸°(preview) ìƒíƒœì´ë©°, ë¹„ë™ê¸° ë Œë”ë§ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤. ì‹¤ìŠµ ì „ì— SDK ì„¤ì¹˜ì™€ API í‚¤ ì„¤ì •ì„ ë¨¼ì € ë³´ì—¬ ì£¼ì„¸ìš”.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "print(\"âœ… API í‚¤ ì„¤ì • ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI, AsyncOpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "async_client = AsyncOpenAI()\n",
        "print(\"âœ… í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Sora ëª¨ë¸ ë¼ì¸ì—… (sora-2 vs sora-2-pro)\n",
        "| ëª¨ë¸ | íŠ¹ì§• | ì¶”ì²œ ì‹œë‚˜ë¦¬ì˜¤ |\n",
        "| --- | --- | --- |\n",
        "| `sora-2` | ë¹ ë¥¸ ë Œë”, í•©ë¦¬ì  ë¹„ìš©, ì‹¤í—˜/ì»¨ì…‰ ì´ˆì•ˆì— ì í•© | ì•„ì´ë””ì–´ íƒìƒ‰, SNS ì½˜í…ì¸ , ë¹ ë¥¸ ë°˜ë³µ |\n",
        "| `sora-2-pro` | ê³ í’ˆì§ˆ, ë” ê¸´ ë Œë” ì‹œê°„/ë¹„ìš©, ì•ˆì •ì  ì›€ì§ì„ | ë§ˆì¼€íŒ…/ì‹œë„¤ë§ˆí‹±, ê³ í•´ìƒë„ ìµœì¢…ë³¸ |\n",
        "\n",
        "> **ê°•ì¡° í¬ì¸íŠ¸**: ë‘ ëª¨ë¸ ëª¨ë‘ ë¹„ë™ê¸° ì‘ì—…ìœ¼ë¡œ ë™ì‘í•˜ë¯€ë¡œ, ì‘ë‹µì—ì„œ ë°›ì€ job IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§„í–‰ ìƒíƒœë¥¼ ì¶”ì í•´ì•¼ í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) ë¹„ë””ì˜¤ ìƒì„± ìš”ì²­ íë¦„(ë¹„ë™ê¸° ë Œë”)\n",
        "ë Œë” ì‘ì—…ì€ `POST /videos`ë¡œ ì‹œì‘í•˜ë©°, ì‘ë‹µì€ ì¦‰ì‹œ ë¹„ë””ì˜¤ê°€ ì•„ë‹Œ **job ê°ì²´**ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. IDë¥¼ ì €ì¥í•´ ë‘ê³  ìƒíƒœê°€ `completed`ê°€ ë  ë•Œê¹Œì§€ ëª¨ë‹ˆí„°ë§í•´ì•¼ í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "video_job = client.videos.create(\n",
        "    model=\"sora-2\",\n",
        "    prompt=\"A video of the words 'Thank you' in sparkling letters\",\n",
        "    size=\"1280x720\",\n",
        "    seconds=8,\n",
        ")\n",
        "\n",
        "print(\"ğŸ“½ï¸ ë Œë” ì‘ì—… ì‹œì‘:\", video_job)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) ìƒíƒœ ëª¨ë‹ˆí„°ë§: í´ë§ vs ì›¹í›…\n",
        "ë Œë” ì§„í–‰ë¥ ì€ `GET /videos/{video_id}`ë¡œ í´ë§í•˜ê±°ë‚˜, ì›¹í›…ì„ ë“±ë¡í•´ `video.completed` / `video.failed` ì´ë²¤íŠ¸ë¥¼ ìˆ˜ì‹ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë¹„ë™ê¸° í´ë§ ìœ í‹¸ë¦¬í‹° (asyncio ì‚¬ìš©)\n",
        "import asyncio\n",
        "\n",
        "async def create_and_poll_async(prompt: str):\n",
        "    job = await async_client.videos.create_and_poll(\n",
        "        model=\"sora-2\",\n",
        "        prompt=prompt,\n",
        "    )\n",
        "    print(\"async ê²°ê³¼ ìƒíƒœ:\", job.status)\n",
        "    return job\n",
        "\n",
        "# asyncio.run(create_and_poll_async(\"A video of a cat on a motorcycle at night\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def wait_for_video(video):\n",
        "    polling_interval = 10\n",
        "    while video.status in {\"queued\", \"in_progress\"}:\n",
        "        bar_length = 30\n",
        "        progress = getattr(video, \"progress\", 0) or 0\n",
        "        filled = int((progress / 100) * bar_length)\n",
        "        bar = \"=\" * filled + \"-\" * (bar_length - filled)\n",
        "        status_text = \"Queued\" if video.status == \"queued\" else \"Processing\"\n",
        "        print(f\"\\r{status_text}: [{bar}] {progress:.1f}%\", end=\"\")\n",
        "        time.sleep(polling_interval)\n",
        "        video = client.videos.retrieve(video.id)\n",
        "    print(\"\\nìµœì¢… ìƒíƒœ:\", video.status)\n",
        "    if video.status == \"failed\":\n",
        "        raise RuntimeError(getattr(getattr(video, \"error\", None), \"message\", \"Video generation failed\"))\n",
        "    return video\n",
        "\n",
        "video_completed = wait_for_video(video_job)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **ì›¹í›… ë°ëª¨ ì„¤ëª…**\n",
        "> - í”„ë¡œì íŠ¸ ì„¤ì •ì—ì„œ Webhook URLì„ ë“±ë¡í•˜ë©´ `video.completed` / `video.failed` ì´ë²¤íŠ¸ë¥¼ ìˆ˜ì‹ í•©ë‹ˆë‹¤.\n",
        "> - í˜ì´ë¡œë“œì—ëŠ” `data.id`ë¡œ ë¹„ë””ì˜¤ IDê°€ í¬í•¨ë˜ë¯€ë¡œ, ì„œë²„ì—ì„œ í›„ì† ë‹¤ìš´ë¡œë“œë¥¼ íŠ¸ë¦¬ê±°í•˜ë©´ ë©ë‹ˆë‹¤.\n",
        "> - ìˆ˜ì—… ì¤‘ì—ëŠ” í´ë§ ë°©ì‹ìœ¼ë¡œ ê°œë…ì„ ë¨¼ì € ë³´ì—¬ì£¼ê³ , ì‹¤ì„œë¹„ìŠ¤ êµ¬í˜„ ì‹œ ì›¹í›…ì„ ì¶”ì²œí•œë‹¤ê³  ì•ˆë‚´í•˜ì„¸ìš”.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ë° ë³´ì¡° ì—ì…‹ í™œìš©\n",
        "ë Œë”ê°€ ì™„ë£Œë˜ë©´ `GET /videos/{video_id}/content`ë¡œ MP4ë¥¼ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆê³ , `variant` íŒŒë¼ë¯¸í„°ë¥¼ ì´ìš©í•´ thumbnail/spritesheet ê°™ì€ ë³´ì¡° ì—ì…‹ë„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "content = client.videos.download_content(video_completed.id)\n",
        "content.write_to_file(\"sora_video.mp4\")\n",
        "print(\"ğŸ‰ ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ -> sora_video.mp4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "thumbnail = client.videos.download_content(video_completed.id, variant=\"thumbnail\")\n",
        "thumbnail.write_to_file(\"sora_thumbnail.webp\")\n",
        "\n",
        "spritesheet = client.videos.download_content(video_completed.id, variant=\"spritesheet\")\n",
        "spritesheet.write_to_file(\"sora_spritesheet.jpg\")\n",
        "print(\"ì¸ë„¤ì¼/ìŠ¤í”„ë¼ì´íŠ¸ì‹œíŠ¸ ì €ì¥ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) ì…ë ¥ ì´ë¯¸ì§€/ë¦¬ë¯¹ìŠ¤ ë“± ê³ ê¸‰ ê¸°ëŠ¥\n",
        "Sora APIëŠ” ì‹œì‘ í”„ë ˆì„ìœ¼ë¡œ ì‚¬ìš©í•  ì´ë¯¸ì§€ ì…ë ¥, ì™„ë£Œëœ ë¹„ë””ì˜¤ì˜ ë¦¬ë¯¹ìŠ¤ ë“± ë°˜ë³µ ì œì‘ ì›Œí¬í”Œë¡œìš°ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```bash\n",
        "curl -X POST \"https://api.openai.com/v1/videos\" \\\n",
        "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  -H \"Content-Type: multipart/form-data\" \\\n",
        "  -F prompt=\"She turns around and smiles, then slowly walks out of the frame.\" \\\n",
        "  -F model=\"sora-2-pro\" \\\n",
        "  -F size=\"1280x720\" \\\n",
        "  -F seconds=\"8\" \\\n",
        "  -F input_reference=\"@sample_720p.jpeg;type=image/jpeg\"\n",
        "```\n",
        "\n",
        "> **ê°•ì˜ í¬ì¸íŠ¸**\n",
        "> - ì…ë ¥ ì´ë¯¸ì§€ëŠ” ìµœì´ˆ í”„ë ˆì„ìœ¼ë¡œ ì‚¬ìš©ë˜ë¯€ë¡œ í•´ìƒë„(`size`)ë¥¼ ë§ì¶°ì•¼ í•©ë‹ˆë‹¤.\n",
        "> - í˜„ì¬ëŠ” ì‚¬ëŒ ì–¼êµ´ì´ ìˆëŠ” ì´ë¯¸ì§€ëŠ” ê±°ë¶€ë˜ë©°, ì €ì‘ê¶Œ/ë¸Œëœë“œ ì†Œì¬ëŠ” ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ì•ˆë‚´í•˜ì„¸ìš”.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ë¦¬ë¯¹ìŠ¤(Remix) ì˜ˆì‹œ\n",
        "ê¸°ì¡´ ë¹„ë””ì˜¤ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒ‰ê°ì´ë‚˜ ì—°ì¶œì„ í•œ ê°€ì§€ì”© ìˆ˜ì •í•˜ëŠ” ë° í™œìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "```bash\n",
        "curl -X POST \"https://api.openai.com/v1/videos/<video_id>/remix\" \\\n",
        "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\n",
        "    \"prompt\": \"Shift the color palette to teal, sand, and rust, with a warm backlight.\"\n",
        "  }'\n",
        "```\n",
        "\n",
        "> **ì‹¤ë¬´ íŒ**: ë¦¬ë¯¹ìŠ¤ëŠ” í•˜ë‚˜ì˜ ëª…í™•í•œ ë³€ê²½ ì‚¬í•­ì„ ì „ë‹¬í• ìˆ˜ë¡ ì•ˆì •ì ìœ¼ë¡œ ê²°ê³¼ê°€ ìœ ì§€ë©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) ë¼ì´ë¸ŒëŸ¬ë¦¬ ê´€ë¦¬ & ì½˜í…ì¸  ì •ì±…\n",
        "- `GET /videos`ë¡œ í”„ë¡œì íŠ¸ ë‚´ ë Œë” íˆìŠ¤í† ë¦¬ë¥¼ í™•ì¸í•˜ê³ , í•„ìš” ì‹œ `limit`, `after`, `order` íŒŒë¼ë¯¸í„°ë¡œ í˜ì´ì§€ë„¤ì´ì…˜í•©ë‹ˆë‹¤.\n",
        "- `DELETE /videos/{video_id}`ë¡œ ì €ì¥ ê³µê°„ì„ ê´€ë¦¬í•˜ì„¸ìš”.\n",
        "- **ì½˜í…ì¸  ê°€ë“œë ˆì¼**: 18ì„¸ ë¯¸ë§Œì—ê²Œ ì í•©í•œ ì½˜í…ì¸ ë§Œ í—ˆìš©ë˜ë©°, ì‹¤ì¡´ ì¸ë¬¼/ì €ì‘ê¶Œ ìºë¦­í„°/ì €ì‘ê¶Œ ìŒì•…ì€ ê±°ë¶€ë©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "video_list = client.videos.list(limit=5)\n",
        "for item in video_list.data:\n",
        "    print(item.id, item.status, item.model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if video_list.data:\n",
        "    client.videos.delete(video_list.data[0].id)\n",
        "    print(\"ğŸ—‘ï¸ ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ ì‚­ì œ ìš”ì²­ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) ë¹„ìš©Â·ì§€ì—°Â·í•œê³„ì™€ ì‹¤ìŠµ ê³¼ì œ\n",
        "- **ë¹„ìš©**: ëª¨ë¸ ì¢…ë¥˜, í•´ìƒë„(`size`), ê¸¸ì´(`seconds`)ì— ë”°ë¼ ë Œë” ì‹œê°„ì´ ê¸¸ì–´ì§€ê³  ë¹„ìš©ì´ ì¦ê°€í•©ë‹ˆë‹¤. ì‘ì—…ë‹¹ ëª‡ ë¶„ ì´ìƒ ì†Œìš”ë  ìˆ˜ ìˆìŒì„ ì•ˆë‚´í•˜ì„¸ìš”.\n",
        "- **í•œê³„**: ì €ì‘ê¶ŒÂ·ì‹¤ì¡´ ì¸ë¬¼Â·íŒê¶Œ ì½˜í…ì¸ ëŠ” ê±°ë¶€ë˜ë©°, í˜„ì¬ ì¸ê°„ ì–¼êµ´ì´ í¬í•¨ëœ ì…ë ¥ ì´ë¯¸ì§€ëŠ” ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
        "- **í”„ë¡œë•ì…˜ íŒ**: ì™„ë£Œ í›„ 1ì‹œê°„ ì´ë‚´ì— íŒŒì¼ì„ ìì²´ ìŠ¤í† ë¦¬ì§€ì— ë°±ì—…í•˜ê³ , ì¥ê¸° ì•„ì¹´ì´ë¹™ì€ ë³„ë„ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ì„¸ìš”.\n",
        "- **ì‹¤ìŠµ ì œì•ˆ**:\n",
        "  1. `sora-2`ì™€ `sora-2-pro` ê²°ê³¼ë¥¼ ë¹„êµí•´ ë³´ê³  í’ˆì§ˆ/ì‹œê°„ ì°¨ì´ë¥¼ ì •ë¦¬\n",
        "  2. ì›¹í›… ì—†ì´ í´ë§ ê¸°ë°˜ ì§„í–‰ë°”ë¥¼ êµ¬í˜„í•œ ë’¤, ì›¹í›… ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ì„ ê·¸ë ¤ë³´ê²Œ í•˜ê¸°\n",
        "  3. ì¸ë„¤ì¼ê³¼ ìŠ¤í”„ë¼ì´íŠ¸ì‹œíŠ¸ë¥¼ í™œìš©í•´ ì»¤ìŠ¤í…€ í”Œë ˆì´ì–´ UI ë””ìì¸ ì‹¤ìŠµ\n",
        "Speech to text\n",
        "==============\n",
        "\n",
        "Learn how to turn audio into text.\n",
        "\n",
        "The Audio API provides two speech to text endpoints:\n",
        "\n",
        "*   `transcriptions`\n",
        "*   `translations`\n",
        "\n",
        "Historically, both endpoints have been backed by our open source [Whisper model](https://openai.com/blog/whisper/) (`whisper-1`). The `transcriptions` endpoint now also supports higher quality model snapshots, with limited parameter support:\n",
        "\n",
        "*   `gpt-4o-mini-transcribe`\n",
        "*   `gpt-4o-transcribe`\n",
        "*   `gpt-4o-transcribe-diarize`\n",
        "\n",
        "All endpoints can be used to:\n",
        "\n",
        "*   Transcribe audio into whatever language the audio is in.\n",
        "*   Translate and transcribe the audio into English.\n",
        "\n",
        "File uploads are currently limited to 25 MB, and the following input file types are supported: `mp3`, `mp4`, `mpeg`, `mpga`, `m4a`, `wav`, and `webm`. Known speaker reference clips for diarization accept the same formats when provided as data URLs.\n",
        "\n",
        "Quickstart\n",
        "----------\n",
        "\n",
        "### Transcriptions\n",
        "\n",
        "The transcriptions API takes as input the audio file you want to transcribe and the desired output file format for the transcription of the audio. All models support the same set of input formats. On output:\n",
        "\n",
        "*   `whisper-1` supports `json`, `text`, `srt`, `verbose_json`, and `vtt`.\n",
        "*   `gpt-4o-transcribe` and `gpt-4o-mini-transcribe` support `json` or plain `text`.\n",
        "*   `gpt-4o-transcribe-diarize` supports `json`, `text`, and `diarized_json` (which adds speaker segments to the response).\n",
        "\n",
        "Transcribe audio\n",
        "\n",
        "```javascript\n",
        "import fs from \"fs\";\n",
        "import OpenAI from \"openai\";\n",
        "\n",
        "const openai = new OpenAI();\n",
        "\n",
        "const transcription = await openai.audio.transcriptions.create({\n",
        "  file: fs.createReadStream(\"/path/to/file/audio.mp3\"),\n",
        "  model: \"gpt-4o-transcribe\",\n",
        "});\n",
        "\n",
        "console.log(transcription.text);\n",
        "```\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "audio_file= open(\"/path/to/file/audio.mp3\", \"rb\")\n",
        "\n",
        "transcription = client.audio.transcriptions.create(\n",
        "    model=\"gpt-4o-transcribe\", \n",
        "    file=audio_file\n",
        ")\n",
        "\n",
        "print(transcription.text)\n",
        "```\n",
        "\n",
        "```bash\n",
        "curl --request POST \\\n",
        "  --url https://api.openai.com/v1/audio/transcriptions \\\n",
        "  --header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  --header 'Content-Type: multipart/form-data' \\\n",
        "  --form file=@/path/to/file/audio.mp3 \\\n",
        "  --form model=gpt-4o-transcribe\n",
        "```\n",
        "\n",
        "By default, the response type will be json with the raw text included.\n",
        "\n",
        "{ \"text\": \"Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger. .... }\n",
        "\n",
        "The Audio API also allows you to set additional parameters in a request. For example, if you want to set the `response_format` as `text`, your request would look like the following:\n",
        "\n",
        "Additional options\n",
        "\n",
        "```javascript\n",
        "import fs from \"fs\";\n",
        "import OpenAI from \"openai\";\n",
        "\n",
        "const openai = new OpenAI();\n",
        "\n",
        "const transcription = await openai.audio.transcriptions.create({\n",
        "  file: fs.createReadStream(\"/path/to/file/speech.mp3\"),\n",
        "  model: \"gpt-4o-transcribe\",\n",
        "  response_format: \"text\",\n",
        "});\n",
        "\n",
        "console.log(transcription.text);\n",
        "```\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "audio_file = open(\"/path/to/file/speech.mp3\", \"rb\")\n",
        "\n",
        "transcription = client.audio.transcriptions.create(\n",
        "    model=\"gpt-4o-transcribe\", \n",
        "    file=audio_file, \n",
        "    response_format=\"text\"\n",
        ")\n",
        "\n",
        "print(transcription.text)\n",
        "```\n",
        "\n",
        "```bash\n",
        "curl --request POST \\\n",
        "  --url https://api.openai.com/v1/audio/transcriptions \\\n",
        "  --header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  --header 'Content-Type: multipart/form-data' \\\n",
        "  --form file=@/path/to/file/speech.mp3 \\\n",
        "  --form model=gpt-4o-transcribe \\\n",
        "  --form response_format=text\n",
        "```\n",
        "\n",
        "The [API Reference](/docs/api-reference/audio) includes the full list of available parameters.\n",
        "\n",
        "`gpt-4o-transcribe` and `gpt-4o-mini-transcribe` support `json` or `text` responses and allow prompts and logprobs. `gpt-4o-transcribe-diarize` adds speaker labels but requires `chunking_strategy` when your audio is longer than 30 seconds (`\"auto\"` is recommended) and does not support prompts, logprobs, or `timestamp_granularities[]`.\n",
        "\n",
        "### Speaker diarization\n",
        "\n",
        "`gpt-4o-transcribe-diarize` produces speaker-aware transcripts. Request the `diarized_json` response format to receive an array of segments with `speaker`, `start`, and `end` metadata. Set `chunking_strategy` (either `\"auto\"` or a Voice Activity Detection configuration) so that the service can split the audio into segments; this is required when the input is longer than 30 seconds.\n",
        "\n",
        "You can optionally supply up to four short audio references with `known_speaker_names[]` and `known_speaker_references[]` to map segments onto known speakers. Provide reference clips between 2â€“10 seconds in any input format supported by the main audio upload; encode them as [data URLs](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/Data_URLs) when using multipart form data.\n",
        "\n",
        "Diarize a meeting recording\n",
        "\n",
        "```javascript\n",
        "import fs from \"fs\";\n",
        "import OpenAI from \"openai\";\n",
        "\n",
        "const openai = new OpenAI();\n",
        "\n",
        "const agentRef = fs.readFileSync(\"agent.wav\").toString(\"base64\");\n",
        "\n",
        "const transcript = await openai.audio.transcriptions.create({\n",
        "  file: fs.createReadStream(\"meeting.wav\"),\n",
        "  model: \"gpt-4o-transcribe-diarize\",\n",
        "  response_format: \"diarized_json\",\n",
        "  chunking_strategy: \"auto\",\n",
        "  extra_body: {\n",
        "    known_speaker_names: [\"agent\"],\n",
        "    known_speaker_references: [\"data:audio/wav;base64,\" + agentRef],\n",
        "  },\n",
        "});\n",
        "\n",
        "for (const segment of transcript.segments) {\n",
        "  console.log(`${segment.speaker}: ${segment.text}`, segment.start, segment.end);\n",
        "}\n",
        "```\n",
        "\n",
        "```python\n",
        "import base64\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "def to_data_url(path: str) -> str:\n",
        "    with open(path, \"rb\") as fh:\n",
        "        return \"data:audio/wav;base64,\" + base64.b64encode(fh.read()).decode(\"utf-8\")\n",
        "\n",
        "with open(\"meeting.wav\", \"rb\") as audio_file:\n",
        "    transcript = client.audio.transcriptions.create(\n",
        "        model=\"gpt-4o-transcribe-diarize\",\n",
        "        file=audio_file,\n",
        "        response_format=\"diarized_json\",\n",
        "        chunking_strategy=\"auto\",\n",
        "        extra_body={\n",
        "            \"known_speaker_names\": [\"agent\"],\n",
        "            \"known_speaker_references\": [to_data_url(\"agent.wav\")],\n",
        "        },\n",
        "    )\n",
        "\n",
        "for segment in transcript.segments:\n",
        "    print(segment.speaker, segment.text, segment.start, segment.end)\n",
        "```\n",
        "\n",
        "```bash\n",
        "curl --request POST \\\n",
        "  --url https://api.openai.com/v1/audio/transcriptions \\\n",
        "  --header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  --header 'Content-Type: multipart/form-data' \\\n",
        "  --form file=@/path/to/file/meeting.wav \\\n",
        "  --form model=gpt-4o-transcribe-diarize \\\n",
        "  --form response_format=diarized_json \\\n",
        "  --form chunking_strategy=auto \\\n",
        "  --form 'known_speaker_names[]=agent' \\\n",
        "  --form 'known_speaker_references[]=data:audio/wav;base64,AAA...'\n",
        "```\n",
        "\n",
        "When `stream=true`, diarized responses emit `transcript.text.segment` events whenever a segment completes. `transcript.text.delta` events include a `segment_id` field, but diarized deltas do not stream partial speaker assignments until each segment is finalized.\n",
        "\n",
        "`gpt-4o-transcribe-diarize` is currently available via `/v1/audio/transcriptions` only and is not yet supported in the Realtime API.\n",
        "\n",
        "### Translations\n",
        "\n",
        "The translations API takes as input the audio file in any of the supported languages and transcribes, if necessary, the audio into English. This differs from our /Transcriptions endpoint since the output is not in the original input language and is instead translated to English text. This endpoint supports only the `whisper-1` model.\n",
        "\n",
        "Translate audio\n",
        "\n",
        "```javascript\n",
        "import fs from \"fs\";\n",
        "import OpenAI from \"openai\";\n",
        "\n",
        "const openai = new OpenAI();\n",
        "\n",
        "const translation = await openai.audio.translations.create({\n",
        "  file: fs.createReadStream(\"/path/to/file/german.mp3\"),\n",
        "  model: \"whisper-1\",\n",
        "});\n",
        "\n",
        "console.log(translation.text);\n",
        "```\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "audio_file = open(\"/path/to/file/german.mp3\", \"rb\")\n",
        "\n",
        "translation = client.audio.translations.create(\n",
        "    model=\"whisper-1\", \n",
        "    file=audio_file,\n",
        ")\n",
        "\n",
        "print(translation.text)\n",
        "```\n",
        "\n",
        "```bash\n",
        "curl --request POST \\\n",
        "  --url https://api.openai.com/v1/audio/translations \\\n",
        "  --header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  --header 'Content-Type: multipart/form-data' \\\n",
        "  --form file=@/path/to/file/german.mp3 \\\n",
        "  --form model=whisper-1 \\\n",
        "```\n",
        "\n",
        "In this case, the inputted audio was german and the outputted text looks like:\n",
        "\n",
        "Hello, my name is Wolfgang and I come from Germany. Where are you heading today?\n",
        "\n",
        "We only support translation into English at this time.\n",
        "\n",
        "Supported languages\n",
        "-------------------\n",
        "\n",
        "We currently [support the following languages](https://github.com/openai/whisper#available-models-and-languages) through both the `transcriptions` and `translations` endpoint:\n",
        "\n",
        "Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh.\n",
        "\n",
        "While the underlying model was trained on 98 languages, we only list the languages that exceeded <50% [word error rate](https://en.wikipedia.org/wiki/Word_error_rate) (WER) which is an industry standard benchmark for speech to text model accuracy. The model will return results for languages not listed above but the quality will be low.\n",
        "\n",
        "We support some ISO 639-1 and 639-3 language codes for GPT-4o based models. For language codes we donâ€™t have, try prompting for specific languages (i.e., â€œOutput in Englishâ€).\n",
        "\n",
        "Timestamps\n",
        "----------\n",
        "\n",
        "By default, the Transcriptions API will output a transcript of the provided audio in text. The [`timestamp_granularities[]` parameter](/docs/api-reference/audio/createTranscription#audio-createtranscription-timestamp_granularities) enables a more structured and timestamped json output format, with timestamps at the segment, word level, or both. This enables word-level precision for transcripts and video edits, which allows for the removal of specific frames tied to individual words.\n",
        "\n",
        "Timestamp options\n",
        "\n",
        "```javascript\n",
        "import fs from \"fs\";\n",
        "import OpenAI from \"openai\";\n",
        "\n",
        "const openai = new OpenAI();\n",
        "\n",
        "const transcription = await openai.audio.transcriptions.create({\n",
        "  file: fs.createReadStream(\"audio.mp3\"),\n",
        "  model: \"whisper-1\",\n",
        "  response_format: \"verbose_json\",\n",
        "  timestamp_granularities: [\"word\"]\n",
        "});\n",
        "\n",
        "console.log(transcription.words);\n",
        "```\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "audio_file = open(\"/path/to/file/speech.mp3\", \"rb\")\n",
        "\n",
        "transcription = client.audio.transcriptions.create(\n",
        "  file=audio_file,\n",
        "  model=\"whisper-1\",\n",
        "  response_format=\"verbose_json\",\n",
        "  timestamp_granularities=[\"word\"]\n",
        ")\n",
        "\n",
        "print(transcription.words)\n",
        "```\n",
        "\n",
        "```bash\n",
        "curl https://api.openai.com/v1/audio/transcriptions \\\n",
        "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  -H \"Content-Type: multipart/form-data\" \\\n",
        "  -F file=\"@/path/to/file/audio.mp3\" \\\n",
        "  -F \"timestamp_granularities[]=word\" \\\n",
        "  -F model=\"whisper-1\" \\\n",
        "  -F response_format=\"verbose_json\"\n",
        "```\n",
        "\n",
        "The `timestamp_granularities[]` parameter is only supported for `whisper-1`.\n",
        "\n",
        "Longer inputs\n",
        "-------------\n",
        "\n",
        "By default, the Transcriptions API only supports files that are less than 25 MB. If you have an audio file that is longer than that, you will need to break it up into chunks of 25 MB's or less or used a compressed audio format. To get the best performance, we suggest that you avoid breaking the audio up mid-sentence as this may cause some context to be lost.\n",
        "\n",
        "One way to handle this is to use the [PyDub open source Python package](https://github.com/jiaaro/pydub) to split the audio:\n",
        "\n",
        "```python\n",
        "from pydub import AudioSegment\n",
        "\n",
        "song = AudioSegment.from_mp3(\"good_morning.mp3\")\n",
        "\n",
        "# PyDub handles time in milliseconds\n",
        "ten_minutes = 10 * 60 * 1000\n",
        "\n",
        "first_10_minutes = song[:ten_minutes]\n",
        "\n",
        "first_10_minutes.export(\"good_morning_10.mp3\", format=\"mp3\")\n",
        "```\n",
        "\n",
        "_OpenAI makes no guarantees about the usability or security of 3rd party software like PyDub._\n",
        "\n",
        "Prompting\n",
        "---------\n",
        "\n",
        "You can use a [prompt](/docs/api-reference/audio/createTranscription#audio/createTranscription-prompt) to improve the quality of the transcripts generated by the Transcriptions API.\n",
        "\n",
        "Prompting\n",
        "\n",
        "```javascript\n",
        "import fs from \"fs\";\n",
        "import OpenAI from \"openai\";\n",
        "\n",
        "const openai = new OpenAI();\n",
        "\n",
        "const transcription = await openai.audio.transcriptions.create({\n",
        "  file: fs.createReadStream(\"/path/to/file/speech.mp3\"),\n",
        "  model: \"gpt-4o-transcribe\",\n",
        "  response_format: \"text\",\n",
        "  prompt:\"The following conversation is a lecture about the recent developments around OpenAI, GPT-4.5 and the future of AI.\",\n",
        "});\n",
        "\n",
        "console.log(transcription.text);\n",
        "```\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "audio_file = open(\"/path/to/file/speech.mp3\", \"rb\")\n",
        "\n",
        "transcription = client.audio.transcriptions.create(\n",
        "  model=\"gpt-4o-transcribe\", \n",
        "  file=audio_file, \n",
        "  response_format=\"text\",\n",
        "  prompt=\"The following conversation is a lecture about the recent developments around OpenAI, GPT-4.5 and the future of AI.\"\n",
        ")\n",
        "\n",
        "print(transcription.text)\n",
        "```\n",
        "\n",
        "```bash\n",
        "curl --request POST \\\n",
        "  --url https://api.openai.com/v1/audio/transcriptions \\\n",
        "  --header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  --header 'Content-Type: multipart/form-data' \\\n",
        "  --form file=@/path/to/file/speech.mp3 \\\n",
        "  --form model=gpt-4o-transcribe \\\n",
        "  --form prompt=\"The following conversation is a lecture about the recent developments around OpenAI, GPT-4.5 and the future of AI.\"\n",
        "```\n",
        "\n",
        "For `gpt-4o-transcribe` and `gpt-4o-mini-transcribe`, you can use the `prompt` parameter to improve the quality of the transcription by giving the model additional context similarly to how you would prompt other GPT-4o models. Prompting is not currently available for `gpt-4o-transcribe-diarize`.\n",
        "\n",
        "Here are some examples of how prompting can help in different scenarios:\n",
        "\n",
        "1.  Prompts can help correct specific words or acronyms that the model misrecognizes in the audio. For example, the following prompt improves the transcription of the words DALLÂ·E and GPT-3, which were previously written as \"GDP 3\" and \"DALI\": \"The transcript is about OpenAI which makes technology like DALLÂ·E, GPT-3, and ChatGPT with the hope of one day building an AGI system that benefits all of humanity.\"\n",
        "2.  To preserve the context of a file that was split into segments, prompt the model with the transcript of the preceding segment. The model uses relevant information from the previous audio, improving transcription accuracy. The `whisper-1` model only considers the final 224 tokens of the prompt and ignores anything earlier. For multilingual inputs, Whisper uses a custom tokenizer. For English-only inputs, it uses the standard GPT-2 tokenizer. Find both tokenizers in the open source [Whisper Python package](https://github.com/openai/whisper/blob/main/whisper/tokenizer.py#L361).\n",
        "3.  Sometimes the model skips punctuation in the transcript. To prevent this, use a simple prompt that includes punctuation: \"Hello, welcome to my lecture.\"\n",
        "4.  The model may also leave out common filler words in the audio. If you want to keep the filler words in your transcript, use a prompt that contains them: \"Umm, let me think like, hmm... Okay, here's what I'm, like, thinking.\"\n",
        "5.  Some languages can be written in different ways, such as simplified or traditional Chinese. The model might not always use the writing style that you want for your transcript by default. You can improve this by using a prompt in your preferred writing style.\n",
        "\n",
        "For `whisper-1`, the model tries to match the style of the prompt, so it's more likely to use capitalization and punctuation if the prompt does too. However, the current prompting system is more limited than our other language models and provides limited control over the generated text.\n",
        "\n",
        "You can find more examples on improving your `whisper-1` transcriptions in the [improving reliability](/docs/guides/speech-to-text#improving-reliability) section.\n",
        "\n",
        "Streaming transcriptions\n",
        "------------------------\n",
        "\n",
        "There are two ways you can stream your transcription depending on your use case and whether you are trying to transcribe an already completed audio recording or handle an ongoing stream of audio and use OpenAI for turn detection.\n",
        "\n",
        "### Streaming the transcription of a completed audio recording\n",
        "\n",
        "If you have an already completed audio recording, either because it's an audio file or you are using your own turn detection (like push-to-talk), you can use our Transcription API with `stream=True` to receive a stream of [transcript events](/docs/api-reference/audio/transcript-text-delta-event) as soon as the model is done transcribing that part of the audio.\n",
        "\n",
        "Stream transcriptions\n",
        "\n",
        "```javascript\n",
        "import fs from \"fs\";\n",
        "import OpenAI from \"openai\";\n",
        "\n",
        "const openai = new OpenAI();\n",
        "\n",
        "const stream = await openai.audio.transcriptions.create({\n",
        "  file: fs.createReadStream(\"/path/to/file/speech.mp3\"),\n",
        "  model: \"gpt-4o-mini-transcribe\",\n",
        "  response_format: \"text\",\n",
        "  stream: true,\n",
        "});\n",
        "\n",
        "for await (const event of stream) {\n",
        "  console.log(event);\n",
        "}\n",
        "```\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "audio_file = open(\"/path/to/file/speech.mp3\", \"rb\")\n",
        "\n",
        "stream = client.audio.transcriptions.create(\n",
        "  model=\"gpt-4o-mini-transcribe\", \n",
        "  file=audio_file, \n",
        "  response_format=\"text\",\n",
        "  stream=True\n",
        ")\n",
        "\n",
        "for event in stream:\n",
        "  print(event)\n",
        "```\n",
        "\n",
        "```bash\n",
        "curl --request POST \\\n",
        "  --url https://api.openai.com/v1/audio/transcriptions \\\n",
        "  --header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  --header 'Content-Type: multipart/form-data' \\\n",
        "  --form file=@example.wav \\\n",
        "  --form model=whisper-1 \\\n",
        "  --form stream=True\n",
        "```\n",
        "\n",
        "You will receive a stream of `transcript.text.delta` events as soon as the model is done transcribing that part of the audio, followed by a `transcript.text.done` event when the transcription is complete that includes the full transcript. When using `response_format=\"diarized_json\"`, the stream also emits `transcript.text.segment` events with speaker labels each time a segment is finalized.\n",
        "\n",
        "Additionally, you can use the `include[]` parameter to include `logprobs` in the response to get the log probabilities of the tokens in the transcription. These can be helpful to determine how confident the model is in the transcription of that particular part of the transcript.\n",
        "\n",
        "Streamed transcription is not supported in `whisper-1`.\n",
        "\n",
        "### Streaming the transcription of an ongoing audio recording\n",
        "\n",
        "In the Realtime API, you can stream the transcription of an ongoing audio recording. To start a streaming session with the Realtime API, create a WebSocket connection with the following URL:\n",
        "\n",
        "```text\n",
        "wss://api.openai.com/v1/realtime?intent=transcription\n",
        "```\n",
        "\n",
        "Below is an example payload for setting up a transcription session:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"type\": \"transcription_session.update\",\n",
        "  \"input_audio_format\": \"pcm16\",\n",
        "  \"input_audio_transcription\": {\n",
        "    \"model\": \"gpt-4o-transcribe\",\n",
        "    \"prompt\": \"\",\n",
        "    \"language\": \"\"\n",
        "  },\n",
        "  \"turn_detection\": {\n",
        "    \"type\": \"server_vad\",\n",
        "    \"threshold\": 0.5,\n",
        "    \"prefix_padding_ms\": 300,\n",
        "    \"silence_duration_ms\": 500,\n",
        "  },\n",
        "  \"input_audio_noise_reduction\": {\n",
        "    \"type\": \"near_field\"\n",
        "  },\n",
        "  \"include\": [\n",
        "    \"item.input_audio_transcription.logprobs\"\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "To stream audio data to the API, append audio buffers:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"type\": \"input_audio_buffer.append\",\n",
        "  \"audio\": \"Base64EncodedAudioData\"\n",
        "}\n",
        "```\n",
        "\n",
        "When in VAD mode, the API will respond with `input_audio_buffer.committed` every time a chunk of speech has been detected. Use `input_audio_buffer.committed.item_id` and `input_audio_buffer.committed.previous_item_id` to enforce the ordering.\n",
        "\n",
        "The API responds with transcription events indicating speech start, stop, and completed transcriptions.\n",
        "\n",
        "The primary resource used by the streaming ASR API is the `TranscriptionSession`:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"object\": \"realtime.transcription_session\",\n",
        "  \"id\": \"string\",\n",
        "  \"input_audio_format\": \"pcm16\",\n",
        "  \"input_audio_transcription\": [{\n",
        "    \"model\": \"whisper-1\" | \"gpt-4o-transcribe\" | \"gpt-4o-mini-transcribe\",\n",
        "    \"prompt\": \"string\",\n",
        "    \"language\": \"string\"\n",
        "  }],\n",
        "  \"turn_detection\": {\n",
        "    \"type\": \"server_vad\",\n",
        "    \"threshold\": \"float\",\n",
        "    \"prefix_padding_ms\": \"integer\",\n",
        "    \"silence_duration_ms\": \"integer\",\n",
        "  } | null,\n",
        "  \"input_audio_noise_reduction\": {\n",
        "    \"type\": \"near_field\" | \"far_field\"\n",
        "  },\n",
        "  \"include\": [\"string\"]\n",
        "}\n",
        "```\n",
        "\n",
        "Authenticate directly through the WebSocket connection using your API key or an ephemeral token obtained from:\n",
        "\n",
        "```text\n",
        "POST /v1/realtime/transcription_sessions\n",
        "```\n",
        "\n",
        "This endpoint returns an ephemeral token (`client_secret`) to securely authenticate WebSocket connections.\n",
        "\n",
        "Improving reliability\n",
        "---------------------\n",
        "\n",
        "One of the most common challenges faced when using Whisper is the model often does not recognize uncommon words or acronyms. Here are some different techniques to improve the reliability of Whisper in these cases:\n",
        "\n",
        "Using the prompt parameter\n",
        "\n",
        "The first method involves using the optional prompt parameter to pass a dictionary of the correct spellings.\n",
        "\n",
        "Because it wasn't trained with instruction-following techniques, Whisper operates more like a base GPT model. Keep in mind that Whisper only considers the first 224 tokens of the prompt.\n",
        "\n",
        "Prompt parameter\n",
        "\n",
        "```javascript\n",
        "import fs from \"fs\";\n",
        "import OpenAI from \"openai\";\n",
        "\n",
        "const openai = new OpenAI();\n",
        "\n",
        "const transcription = await openai.audio.transcriptions.create({\n",
        "  file: fs.createReadStream(\"/path/to/file/speech.mp3\"),\n",
        "  model: \"whisper-1\",\n",
        "  response_format: \"text\",\n",
        "  prompt:\"ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.\",\n",
        "});\n",
        "\n",
        "console.log(transcription.text);\n",
        "```\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "audio_file = open(\"/path/to/file/speech.mp3\", \"rb\")\n",
        "\n",
        "transcription = client.audio.transcriptions.create(\n",
        "  model=\"whisper-1\", \n",
        "  file=audio_file, \n",
        "  response_format=\"text\",\n",
        "  prompt=\"ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.\"\n",
        ")\n",
        "\n",
        "print(transcription.text)\n",
        "```\n",
        "\n",
        "```bash\n",
        "curl --request POST \\\n",
        "  --url https://api.openai.com/v1/audio/transcriptions \\\n",
        "  --header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  --header 'Content-Type: multipart/form-data' \\\n",
        "  --form file=@/path/to/file/speech.mp3 \\\n",
        "  --form model=whisper-1 \\\n",
        "  --form prompt=\"ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.\"\n",
        "```\n",
        "\n",
        "While it increases reliability, this technique is limited to 224 tokens, so your list of SKUs needs to be relatively small for this to be a scalable solution.\n",
        "\n",
        "Post-processing with GPT-4\n",
        "\n",
        "The second method involves a post-processing step using GPT-4 or GPT-3.5-Turbo.\n",
        "\n",
        "We start by providing instructions for GPT-4 through the `system_prompt` variable. Similar to what we did with the prompt parameter earlier, we can define our company and product names.\n",
        "\n",
        "Post-processing\n",
        "\n",
        "```javascript\n",
        "const systemPrompt = `\n",
        "You are a helpful assistant for the company ZyntriQix. Your task is \n",
        "to correct any spelling discrepancies in the transcribed text. Make \n",
        "sure that the names of the following products are spelled correctly: \n",
        "ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, \n",
        "OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., \n",
        "Q.U.A.R.T.Z., F.L.I.N.T. Only add necessary punctuation such as \n",
        "periods, commas, and capitalization, and use only the context provided.\n",
        "`;\n",
        "\n",
        "const transcript = await transcribe(audioFile);\n",
        "const completion = await openai.chat.completions.create({\n",
        "model: \"gpt-4.1\",\n",
        "temperature: temperature,\n",
        "messages: [\n",
        "  {\n",
        "    role: \"system\",\n",
        "    content: systemPrompt\n",
        "  },\n",
        "  {\n",
        "    role: \"user\",\n",
        "    content: transcript\n",
        "  }\n",
        "],\n",
        "store: true,\n",
        "});\n",
        "\n",
        "console.log(completion.choices[0].message.content);\n",
        "```\n",
        "\n",
        "```python\n",
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant for the company ZyntriQix. Your task is to correct \n",
        "any spelling discrepancies in the transcribed text. Make sure that the names of \n",
        "the following products are spelled correctly: ZyntriQix, Digique Plus, \n",
        "CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal \n",
        "Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T. Only add necessary \n",
        "punctuation such as periods, commas, and capitalization, and use only the \n",
        "context provided.\n",
        "\"\"\"\n",
        "\n",
        "def generate_corrected_transcript(temperature, system_prompt, audio_file):\n",
        "  response = client.chat.completions.create(\n",
        "      model=\"gpt-4.1\",\n",
        "      temperature=temperature,\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": system_prompt\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": transcribe(audio_file, \"\")\n",
        "          }\n",
        "      ]\n",
        "  )\n",
        "  return completion.choices[0].message.content\n",
        "corrected_text = generate_corrected_transcript(\n",
        "  0, system_prompt, fake_company_filepath\n",
        ")\n",
        "```\n",
        "\n",
        "If you try this on your own audio file, you'll see that GPT-4 corrects many misspellings in the transcript. Due to its larger context window, this method might be more scalable than using Whisper's prompt parameter. It's also more reliable, as GPT-4 can be instructed and guided in ways that aren't possible with Whisper due to its lack of instruction following.\n",
        "\n",
        "Was this page useful? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ğŸ’¡ **ê°•ì˜ ë§ˆë¬´ë¦¬**\n",
        "> - ì‹¤ìŠµ ë…¸íŠ¸(`02-Sora-Video-Practice.ipynb` ì˜ˆì •)ì— TODO í˜•íƒœë¡œ ë™ì¼í•œ ë‹¨ê³„(ìƒì„± â†’ ëª¨ë‹ˆí„°ë§ â†’ ë‹¤ìš´ë¡œë“œ)ë¥¼ ì œê³µí•´ í•™ìƒë“¤ì´ ì§ì ‘ êµ¬í˜„í•˜ë„ë¡ ìœ ë„í•˜ì„¸ìš”.\n",
        "> - ê¸´ ë Œë” ì‹œê°„ì´ ì˜ˆìƒë˜ë¯€ë¡œ, ê°•ì˜ ì „ ìƒì„±í•œ ìƒ˜í”Œ ì˜ìƒì„ ì¤€ë¹„í•´ ê²°ê³¼ ë¹„êµ/í† ë¡  ì‹œê°„ì„ í™•ë³´í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
