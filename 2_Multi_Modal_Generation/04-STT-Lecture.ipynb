{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ™ï¸ Speech-to-Text (STT) ê°•ì˜ ë…¸íŠ¸\n",
        "\n",
        "> **ì£¼ì˜:** Colab ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘í•˜ë©´ `OPENAI_API_KEY`ë¥¼ ë‹¤ì‹œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) í™˜ê²½ ì¤€ë¹„ ë° API í‚¤ ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "OPENAI_API_KEY = None\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "except Exception:\n",
        "    print(\"Colab ë©”ë‰´ì˜ ë³´ì•ˆ ë¹„ë°€(ì—´ì‡  ì•„ì´ì½˜)ì—ì„œ í™˜ê²½ ë³€ìˆ˜ê°€ ì œëŒ€ë¡œ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "print(\"âœ… API í‚¤ ì„¤ì • ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from pathlib import Path\n",
        "\n",
        "client = OpenAI()\n",
        "print(\"âœ… í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) STT ê°œìš” ë° ì§€ì› í˜•ì‹\n",
        "\n",
        "**Speech-to-Text(STT)**ëŠ” ì‚¬ëŒì˜ ë§ì†Œë¦¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
        "\n",
        "**ì‚¬ìš© ì˜ˆì‹œ**\n",
        "- ìŒì„± ê¸°ë°˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ\n",
        "- ì‹¤ì‹œê°„ ìë§‰ ìƒì„±\n",
        "- ìŒì„± ë©”ëª¨ ë³€í™˜\n",
        "\n",
        "**OpenAI Audio APIëŠ” ë‘ ê°€ì§€ speech-to-text ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤:**\n",
        "- **`transcriptions`**: ì˜¤ë””ì˜¤ë¥¼ í•´ë‹¹ ì–¸ì–´ì˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
        "- **`translations`**: ì˜¤ë””ì˜¤ë¥¼ ì˜ì–´ í…ìŠ¤íŠ¸ë¡œ ë²ˆì—­ ë° ë³€í™˜\n",
        "\n",
        "**ì§€ì› ëª¨ë¸**\n",
        "- **`whisper-1`**: ì˜¤í”ˆì†ŒìŠ¤ Whisper ëª¨ë¸ ê¸°ë°˜ (ê¸°ë³¸ ëª¨ë¸)\n",
        "- **`gpt-4o-transcribe`**: ê³ í’ˆì§ˆ ì „ì‚¬ ëª¨ë¸\n",
        "- **`gpt-4o-mini-transcribe`**: ê²½ëŸ‰ ê³ í’ˆì§ˆ ì „ì‚¬ ëª¨ë¸\n",
        "- **`gpt-4o-transcribe-diarize`**: ë°œí™”ì êµ¬ë¶„ ê¸°ëŠ¥ í¬í•¨ ëª¨ë¸\n",
        "\n",
        "**íŒŒì¼ ì œí•œì‚¬í•­**\n",
        "- **íŒŒì¼ í¬ê¸°**: ìµœëŒ€ 25MB\n",
        "- **ì§€ì› í˜•ì‹**: `mp3`, `mp4`, `mpeg`, `mpga`, `m4a`, `wav`, `webm`\n",
        "\n",
        "**ì§€ì› ì–¸ì–´**\n",
        "- **í•œêµ­ì–´**, ì˜ì–´ ë“± 98ê°œ ì–¸ì–´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Transcriptions ì—”ë“œí¬ì¸íŠ¸ ê¸°ë³¸ ì‚¬ìš©ë²•\n",
        "\n",
        "### ê¸°ë³¸ íë¦„\n",
        "- `client.audio.transcriptions.create`ë¡œ ì˜¤ë””ì˜¤ íŒŒì¼ê³¼ ëª¨ë¸ì„ ì§€ì •í•´ í…ìŠ¤íŠ¸ë¥¼ ì–»ìŠµë‹ˆë‹¤.\n",
        "- ì‹¤ìŠµ ëª¨ë¸: `gpt-4o-mini-transcribe`\n",
        "- ì‘ë‹µì€ ê¸°ë³¸ì ìœ¼ë¡œ JSON(`response.text`)ì´ë©°, `response_format=\"text\"`ë¡œ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### ì£¼ìš” íŒŒë¼ë¯¸í„°\n",
        "- **`file`**: ë³€í™˜í•  ì˜¤ë””ì˜¤ íŒŒì¼ ê°ì²´ (íŒŒì¼ ì´ë¦„ì´ ì•„ë‹˜)\n",
        "- **`model`**: ì‚¬ìš©í•  ëª¨ë¸ (`gpt-4o-transcribe`, `gpt-4o-mini-transcribe`, `whisper-1` ë“±)\n",
        "- **`language`**: ì…ë ¥ ì˜¤ë””ì˜¤ì˜ ì–¸ì–´ (ISO-639-1 í˜•ì‹ì€ ì •ë°€ë„ì™€ ì§€ì—°ì„ ê°œì„ í•©ë‹ˆë‹¤, ì˜ˆ: `\"ko\"`, `\"en\"`)\n",
        "    - [ISO-639-1 í˜•ì‹](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes)\n",
        "- **`prompt`**: (ì„ íƒ) ëª¨ë¸ì˜ ìŠ¤íƒ€ì¼ì„ ì•ˆë‚´í•˜ê±°ë‚˜ ì´ì „ ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ê³„ì†í•˜ê¸° ìœ„í•œ í…ìŠ¤íŠ¸, audio languageì™€ ë§¤ì¹˜í•´ì•¼í•¨\n",
        "- **`response_format`**: ì¶œë ¥ í˜•ì‹ (`json`, `text`, `srt`, `verbose_json`, `vtt`, `diarized_json`)\n",
        "- **`temperature`**: 0~1 ì‚¬ì´ì˜ ìƒ˜í”Œë§ temperature (ê¸°ë³¸ê°’: 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transcribe_basic(audio_path):\n",
        "    \"\"\"ê¸°ë³¸ ì „ì‚¬ í•¨ìˆ˜ - gpt-4o-transcribe ëª¨ë¸ ì‚¬ìš©\"\"\"\n",
        "    path = Path(audio_path)\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}\")\n",
        "\n",
        "    with path.open(\"rb\") as audio_file:\n",
        "        transcription = client.audio.transcriptions.create(\n",
        "            model=\"gpt-4o-transcribe\",\n",
        "            file=audio_file,\n",
        "        )\n",
        "    \n",
        "    print(\"ì „ì‚¬ ê²°ê³¼ ì˜ˆì‹œ:\\n\", transcription.text[:200])\n",
        "    return transcription.text\n",
        "\n",
        "# ì‚¬ìš© ì˜ˆì‹œ:\n",
        "# transcribe_basic(\"data/sample_lecture.mp3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) ê³ ê¸‰ ì˜µì…˜: ì‘ë‹µ í¬ë§· Â· í”„ë¡¬í”„íŠ¸ Â· ë¡œê·¸í™•ë¥ \n",
        "\n",
        "### ì‘ë‹µ í¬ë§· (response_format)\n",
        "- **`json`**: ê¸°ë³¸ JSON í˜•ì‹ (í…ìŠ¤íŠ¸ í¬í•¨)\n",
        "- **`text`**: ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜\n",
        "- **`verbose_json`**: íƒ€ì„ìŠ¤íƒ¬í”„ ë“± ì¶”ê°€ ë©”íƒ€ë°ì´í„° í¬í•¨\n",
        "- **`diarized_json`**: ë°œí™”ì êµ¬ë¶„ ì •ë³´ í¬í•¨ (gpt-4o-transcribe-diarize ì „ìš©)\n",
        "- **`srt`**: ìë§‰ íŒŒì¼ í˜•ì‹\n",
        "- **`vtt`**: WebVTT ìë§‰ í˜•ì‹\n",
        "\n",
        "### í”„ë¡¬í”„íŠ¸ (prompt)\n",
        "- ëª¨ë¸ì´ ì˜ëª» ì¸ì‹í•˜ëŠ” ìš©ì–´/ê³ ìœ ëª…ì‚¬ë¥¼ ì•ˆë‚´í•´ ì •í™•ë„ í–¥ìƒ\n",
        "- ì´ì „ ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ì˜ í…ìŠ¤íŠ¸ë¥¼ ì „ë‹¬í•´ ë§¥ë½ ìœ ì§€\n",
        "- `whisper-1`ì€ ì²˜ìŒ 224 í† í°ë§Œ ê³ ë ¤\n",
        "- `gpt-4o-transcribe`ëŠ” ë” ê¸´ í”„ë¡¬í”„íŠ¸ ì§€ì›\n",
        "\n",
        "### ë¡œê·¸í™•ë¥  (logprobs)\n",
        "- `include=[\"logprobs\"]`: í† í°ë³„ í™•ë¥ ì„ í™•ì¸í•´ ì‹ ë¢°ë„ ë¶„ì„\n",
        "- `gpt-4o-transcribe` ë° `gpt-4o-mini-transcribe`ì—ì„œ ì§€ì›\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transcribe_with_options(audio_path, *, prompt=None, response_format=\"json\", language=None):\n",
        "    \"\"\"ê³ ê¸‰ ì˜µì…˜ì„ ì‚¬ìš©í•œ ì „ì‚¬ í•¨ìˆ˜\"\"\"\n",
        "    path = Path(audio_path)\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}\")\n",
        "\n",
        "    kwargs = {\n",
        "        \"model\": \"gpt-4o-transcribe\",\n",
        "        \"response_format\": response_format,\n",
        "    }\n",
        "    \n",
        "    if prompt:\n",
        "        kwargs[\"prompt\"] = prompt\n",
        "    if language:\n",
        "        kwargs[\"language\"] = language  # ISO-639-1 í˜•ì‹ (ì˜ˆ: \"ko\", \"en\")\n",
        "    if response_format == \"json\":\n",
        "        kwargs[\"include\"] = [\"logprobs\"]\n",
        "\n",
        "    with path.open(\"rb\") as audio_file:\n",
        "        transcription = client.audio.transcriptions.create(\n",
        "            file=audio_file,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    print(\"í…ìŠ¤íŠ¸ ì˜ˆì‹œ:\", transcription.text[:120])\n",
        "    if response_format == \"json\" and getattr(transcription, \"logprobs\", None):\n",
        "        print(\"ì²« í† í° ë¡œê·¸í™•ë¥ :\", transcription.logprobs[0])\n",
        "    \n",
        "    return transcription\n",
        "\n",
        "# ì‚¬ìš© ì˜ˆì‹œ:\n",
        "# transcribe_with_options(\n",
        "#     \"data/sample_lecture.mp3\", \n",
        "#     prompt=\"This is a talk about GPT-4o and Sora.\",\n",
        "#     language=\"en\"\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) ë°œí™”ì êµ¬ë¶„(Diarization)\n",
        "\n",
        "### ê°œìš”\n",
        "`gpt-4o-transcribe-diarize` ëª¨ë¸ì€ íšŒì˜ë‚˜ ëŒ€í™”ì—ì„œ ê° ë°œí™”ìë¥¼ êµ¬ë¶„í•˜ì—¬ ì „ì‚¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### ì£¼ìš” íŠ¹ì§•\n",
        "- `response_format=\"diarized_json\"`ì„ í†µí•´ `speaker`, `start`, `end` ì •ë³´ë¥¼ í¬í•¨í•œ ì„¸ê·¸ë¨¼íŠ¸ ì œê³µ\n",
        "- 30ì´ˆ ì´ìƒ íŒŒì¼ì€ `chunking_strategy=\"auto\"` ì§€ì • í•„ìˆ˜ (ê¶Œì¥)\n",
        "- `known_speaker_names[]`ì™€ `known_speaker_references[]`ë¡œ ì‚¬ì „ ë“±ë¡í•œ ìŒì„±ê³¼ ë°œí™”ì ë§¤ì¹­ ê°€ëŠ¥\n",
        "- ìŠ¤íŠ¸ë¦¬ë°(`stream=True`) ì‹œ ì„¸ê·¸ë¨¼íŠ¸ ì™„ë£Œ ì‹œì ë§ˆë‹¤ `transcript.text.segment` ì´ë²¤íŠ¸ ë°œìƒ\n",
        "\n",
        "### ì œí•œì‚¬í•­\n",
        "- `prompt`, `logprobs`, `timestamp_granularities[]` íŒŒë¼ë¯¸í„° ë¯¸ì§€ì›\n",
        "- Realtime APIì—ì„œëŠ” ì•„ì§ ì§€ì›ë˜ì§€ ì•ŠìŒ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def diarize_demo(audio_path, known_speakers=None):\n",
        "    \"\"\"ë°œí™”ì êµ¬ë¶„ ì „ì‚¬ í•¨ìˆ˜\"\"\"\n",
        "    path = Path(audio_path)\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}\")\n",
        "\n",
        "    kwargs = {\n",
        "        \"model\": \"gpt-4o-transcribe-diarize\",\n",
        "        \"response_format\": \"diarized_json\",\n",
        "        \"chunking_strategy\": \"auto\",\n",
        "    }\n",
        "    \n",
        "    # ì•Œë ¤ì§„ ë°œí™”ì ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€\n",
        "    if known_speakers:\n",
        "        import base64\n",
        "        known_speaker_names = []\n",
        "        known_speaker_references = []\n",
        "        \n",
        "        for name, ref_path in known_speakers.items():\n",
        "            known_speaker_names.append(name)\n",
        "            with open(ref_path, \"rb\") as f:\n",
        "                ref_data = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "                known_speaker_references.append(f\"data:audio/wav;base64,{ref_data}\")\n",
        "        \n",
        "        kwargs[\"extra_body\"] = {\n",
        "            \"known_speaker_names\": known_speaker_names,\n",
        "            \"known_speaker_references\": known_speaker_references,\n",
        "        }\n",
        "\n",
        "    with path.open(\"rb\") as audio_file:\n",
        "        transcript = client.audio.transcriptions.create(\n",
        "            file=audio_file,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    print(\"ë°œí™”ìë³„ ì „ì‚¬ ê²°ê³¼ (ì²˜ìŒ 5ê°œ ì„¸ê·¸ë¨¼íŠ¸):\")\n",
        "    for segment in transcript.segments[:5]:\n",
        "        print(f\"{segment.speaker}: {segment.text} ({segment.start:.1f}s ~ {segment.end:.1f}s)\")\n",
        "    \n",
        "    return transcript\n",
        "\n",
        "# ì‚¬ìš© ì˜ˆì‹œ:\n",
        "# diarize_demo(\"data/meeting.wav\")\n",
        "# \n",
        "# ì•Œë ¤ì§„ ë°œí™”ìê°€ ìˆëŠ” ê²½ìš°:\n",
        "# diarize_demo(\"data/meeting.wav\", known_speakers={\n",
        "#     \"agent\": \"data/agent.wav\",\n",
        "#     \"customer\": \"data/customer.wav\"\n",
        "# })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Translations ì—”ë“œí¬ì¸íŠ¸\n",
        "\n",
        "### ê°œìš”\n",
        "`translations` ì—”ë“œí¬ì¸íŠ¸ëŠ” ë¹„ì˜ì–´ ìŒì„±ì„ ì˜ì–´ í…ìŠ¤íŠ¸ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.\n",
        "\n",
        "### íŠ¹ì§•\n",
        "- **`whisper-1` ëª¨ë¸ ì „ìš©**: í˜„ì¬ `whisper-1`ë§Œ ì§€ì›\n",
        "- **ì¶œë ¥ ì–¸ì–´**: ì›ë¬¸ ì–¸ì–´ê°€ ë¬´ì—‡ì´ë“  ì¶œë ¥ì€ í•­ìƒ ì˜ì–´\n",
        "- **ì…ë ¥ ì–¸ì–´**: ì§€ì›ë˜ëŠ” ëª¨ë“  ì–¸ì–´ì˜ ì˜¤ë””ì˜¤ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­ ê°€ëŠ¥\n",
        "\n",
        "### ì‚¬ìš© ì‚¬ë¡€\n",
        "- ë‹¤êµ­ì–´ íšŒì˜ë¡ì„ ì˜ì–´ë¡œ í†µì¼\n",
        "- ê¸€ë¡œë²Œ ì½˜í…ì¸  ìë§‰ ìƒì„±\n",
        "- ì–¸ì–´ ì¥ë²½ ì—†ëŠ” ìŒì„± ë²ˆì—­ ì„œë¹„ìŠ¤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate_demo(audio_path):\n",
        "    \"\"\"ë¹„ì˜ì–´ ì˜¤ë””ì˜¤ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
        "    path = Path(audio_path)\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}\")\n",
        "\n",
        "    with path.open(\"rb\") as audio_file:\n",
        "        translation = client.audio.translations.create(\n",
        "            model=\"whisper-1\",\n",
        "            file=audio_file,\n",
        "        )\n",
        "\n",
        "    print(\"ì˜ì–´ ë²ˆì—­ ê²°ê³¼:\", translation.text[:200])\n",
        "    return translation.text\n",
        "\n",
        "# ì‚¬ìš© ì˜ˆì‹œ:\n",
        "# translate_demo(\"data/german.mp3\")\n",
        "# translate_demo(\"data/korean_speech.wav\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸/í›„ì²˜ë¦¬ ì „ëµ\n",
        "\n",
        "### í”„ë¡¬í”„íŠ¸ í™œìš© ë°©ë²•\n",
        "\n",
        "#### 1. ê³ ìœ ëª…ì‚¬/ì•½ì–´ ë³´ì •\n",
        "`prompt` íŒŒë¼ë¯¸í„°ì— ì˜¬ë°”ë¥¸ ì² ì ë¦¬ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ì—¬ ì¸ì‹ ì •í™•ë„ í–¥ìƒ\n",
        "\n",
        "```python\n",
        "prompt = \"DALLÂ·E, GPT-3, ChatGPT, OpenAI\"\n",
        "```\n",
        "\n",
        "#### 2. ì´ì „ ì„¸ê·¸ë¨¼íŠ¸ ë§¥ë½ ìœ ì§€\n",
        "ê¸´ ì˜¤ë””ì˜¤ë¥¼ ë¶„í• í•  ë•Œ ì´ì „ ì„¸ê·¸ë¨¼íŠ¸ì˜ í…ìŠ¤íŠ¸ë¥¼ promptë¡œ ì „ë‹¬\n",
        "\n",
        "#### 3. êµ¬ë‘ì /ìŠ¤íƒ€ì¼ ì•ˆë‚´\n",
        "- êµ¬ë‘ì  í¬í•¨: `prompt=\"Hello, welcome to my lecture.\"`\n",
        "- í•„ëŸ¬ ë‹¨ì–´ ìœ ì§€: `prompt=\"Umm, let me think like, hmm...\"`\n",
        "- ì–¸ì–´ ìŠ¤íƒ€ì¼ ì§€ì •: ê°„ì²´/ë²ˆì²´ ì¤‘êµ­ì–´ ë“±\n",
        "\n",
        "#### 4. ì œí•œì‚¬í•­\n",
        "- `whisper-1`: ì²˜ìŒ 224 í† í°ë§Œ ê³ ë ¤ (ë‹¤êµ­ì–´ëŠ” ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì € ì‚¬ìš©)\n",
        "- `gpt-4o-transcribe`: ë” ê¸´ í”„ë¡¬í”„íŠ¸ ì§€ì›, ë” ë‚˜ì€ ì§€ì‹œ ë”°ë¥´ê¸°\n",
        "\n",
        "### GPT-4 ê¸°ë°˜ í›„ì²˜ë¦¬\n",
        "\n",
        "ì „ì‚¬ ê²°ê³¼ë¥¼ GPT-4ì— ì „ë‹¬í•˜ì—¬ í’ˆì§ˆ í–¥ìƒ:\n",
        "\n",
        "1. **ì˜¤íƒ€ ìˆ˜ì •**: ì˜ëª» ì¸ì‹ëœ ë‹¨ì–´ ë³´ì •\n",
        "2. **ë§ì¶¤ë²• ë³´ì •**: ë¬¸ë²• ë° ì² ì êµì •\n",
        "3. **ìŠ¤íƒ€ì¼ í†µì¼**: ì¼ê´€ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
        "4. **í™•ì¥ì„±**: ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê°€ëŠ¥\n",
        "\n",
        "```python\n",
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant. Your task is to correct any spelling \n",
        "discrepancies in the transcribed text. Make sure that the names of \n",
        "the following products are spelled correctly: DALLÂ·E, GPT-3, ChatGPT.\n",
        "Only add necessary punctuation such as periods, commas, and capitalization.\n",
        "\"\"\"\n",
        "\n",
        "transcript = transcribe_audio(audio_file)\n",
        "corrected = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": transcript}\n",
        "    ]\n",
        ")\n",
        "```\n",
        "\n",
        "> **ì‹¤ìŠµ ì—°ê²°**: í•™ìƒë“¤ì—ê²Œ prompt íŠœë‹ê³¼ í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì§ì ‘ ì‹¤í—˜í•˜ê²Œ í•˜ê³ , í’ˆì§ˆ í–¥ìƒ ì „/í›„ ë¹„êµë¥¼ ê³¼ì œë¡œ ì œì‹œí•´ ë³´ì„¸ìš”.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) ì‹¤ì „ í™œìš© ì˜ˆì‹œ\n",
        "\n",
        "### íšŒì˜ë¡ ìë™ ì‘ì„±\n",
        "```python\n",
        "# ë°œí™”ì êµ¬ë¶„ì„ í™œìš©í•œ íšŒì˜ë¡ ìƒì„±\n",
        "transcript = diarize_demo(\"data/meeting.wav\")\n",
        "for segment in transcript.segments:\n",
        "    print(f\"[{segment.speaker}] {segment.text}\")\n",
        "```\n",
        "\n",
        "### ë‹¤êµ­ì–´ ì½˜í…ì¸  ìë§‰ ìƒì„±\n",
        "```python\n",
        "# í•œêµ­ì–´ ì˜¤ë””ì˜¤ë¥¼ ì˜ì–´ ìë§‰ìœ¼ë¡œ ë³€í™˜\n",
        "english_subtitle = translate_demo(\"data/korean_video.wav\")\n",
        "```\n",
        "\n",
        "### ì˜ìƒ í¸ì§‘ ìë™í™”\n",
        "```python\n",
        "# ë‹¨ì–´ ë‹¨ìœ„ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ì •í™•í•œ í¸ì§‘ ì§€ì  ì°¾ê¸°\n",
        "transcription = transcribe_with_timestamps(\"data/video_audio.wav\")\n",
        "# íŠ¹ì • ë‹¨ì–´ì˜ ìœ„ì¹˜ë¥¼ ì°¾ì•„ í•´ë‹¹ í”„ë ˆì„ í¸ì§‘\n",
        "```\n",
        "\n",
        "### ì‹¤ì‹œê°„ ìë§‰ ì„œë¹„ìŠ¤\n",
        "- ìŠ¤íŠ¸ë¦¬ë° APIë¥¼ í™œìš©í•œ ì‹¤ì‹œê°„ ìë§‰ ìƒì„±\n",
        "- Realtime APIë¡œ WebSocket ê¸°ë°˜ ì‹¤ì‹œê°„ ì „ì‚¬\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 12) ìƒ˜í”Œ ë°ì´í„°ì…‹\n",
        "\n",
        "### ì±„ìš©ë©´ì ‘ ì¸í„°ë·° ë°ì´í„°\n",
        "- **ë§í¬**: [AI Hub - ì±„ìš©ë©´ì ‘ ë°ì´í„°ì…‹](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=data&dataSetSn=71592)\n",
        "- **ìš©ë„**: ë°œí™”ì êµ¬ë¶„(Diarization) ì‹¤ìŠµì— ì í•©\n",
        "- **í˜•ì‹**: ë©´ì ‘ê´€ê³¼ ì§€ì›ìì˜ ëŒ€í™” ë…¹ìŒ\n",
        "\n",
        "### ì˜¤ë””ì˜¤ íŒŒì¼ ë¶„í•  ì˜ˆì‹œ ì½”ë“œ\n",
        "ì•„ë˜ ì½”ë“œëŠ” ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì •í•´ì§„ ì‹œê°„ì— ë”°ë¼ ë¶„ì ˆí•˜ì—¬ ë³„ë„ì˜ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤:\n",
        "\n",
        "```python\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ\n",
        "audio = AudioSegment.from_wav(\"interview.wav\")\n",
        "\n",
        "# 5ë¶„(300ì´ˆ) ë‹¨ìœ„ë¡œ ë¶„í• \n",
        "segment_length = 5 * 60 * 1000  # ë°€ë¦¬ì´ˆ\n",
        "\n",
        "for i, start_time in enumerate(range(0, len(audio), segment_length)):\n",
        "    segment = audio[start_time:start_time + segment_length]\n",
        "    segment.export(f\"interview_part_{i+1}.wav\", format=\"wav\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ğŸ’¡ **ê°•ì˜ ë§ˆë¬´ë¦¬**\n",
        "> - í”„ë¡¬í”„íŠ¸ íŠœë‹ê³¼ í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œ í™œìš©í•  ìˆ˜ ìˆëŠ” í’ˆì§ˆ í–¥ìƒ ê¸°ë²•ì„ ê°•ì¡°í•˜ì„¸ìš”.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ì°¸ê³  ìë£Œ\n",
        "- https://platform.openai.com/docs/guides/speech-to-text\n",
        "- https://platform.openai.com/docs/api-reference/audio"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm_api",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
