{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“˜ Text Generation-Lecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **ì£¼ì˜:** Colab ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘í•˜ë©´ í™˜ê²½ ë³€ìˆ˜ê°€ ì´ˆê¸°í™”ë˜ë¯€ë¡œ `OPENAI_API_KEY`ë¥¼ ë‹¤ì‹œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ì§„í–‰ ìˆœì„œ\n",
        "1. í™˜ê²½ ì¤€ë¹„ ë° API key ì„¤ì •\n",
        "2. GPT-5 ê³„ì—´ ëª¨ë¸ ê°œìš” ë° ì£¼ìš” íŒŒë¼ë¯¸í„°\n",
        "3. ëª¨ë¸ë³„ ì¶œë ¥ ì°¨ì´ ë¹„êµ ì‹¤ìŠµ\n",
        "4. Text Generation ê°œë…ê³¼ instruction/input êµ¬ì¡°\n",
        "5. reasoning.effort Â· text.verbosity Â· max_output_tokens ì œì–´\n",
        "6. ë©€í‹°í„´ ëŒ€í™” êµ¬ì„± ì‹¤ìŠµ (ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë°ëª¨ í¬í•¨)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) í™˜ê²½ ì¤€ë¹„ ë° API í‚¤ ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### API í‚¤ ì„¤ì • ë° Client ìƒì„±\n",
        "- Colab ë©”ë‰´ `ë³´ì•ˆ ë¹„ë°€(ì—´ì‡  ì•„ì´ì½˜)`ì— `OPENAI_API_KEY`ë¥¼ ì €ì¥í•´ ë‘ì—ˆë‹¤ë©´ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "- ì•„ë˜ì˜ ì½”ë“œì—ì„œ ì˜¤ë¥˜ê°€ ë‚œë‹¤ë©´ API í‚¤ì˜ ì˜¤ë¥˜ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "OPENAI_API_KEY = None\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "except Exception:\n",
        "    print(\"Colab ë©”ë‰´ì˜ ë³´ì•ˆ ë¹„ë°€(ì—´ì‡  ì•„ì´ì½˜)ì—ì„œ í™˜ê²½ ë³€ìˆ˜ê°€ ì œëŒ€ë¡œ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "print(\"âœ… API í‚¤ ì„¤ì • ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "print(\"âœ… í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OpenAI API ì‹œì‘í•˜ê¸°\n",
        "- Responses APIë¥¼ ì‚¬ìš©í•˜ë©´ ì±—GPTì²˜ëŸ¼ í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸(ì½”ë“œ, ìˆ˜ì‹, JSON, ìì—°ì–´ ë“±)ë¥¼ ê°„í¸í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.responses.create(\n",
        "    model=\"gpt-5-nano\",\n",
        "    instructions=\"You are a helpful assistant. Reply in Korean.\",\n",
        "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ëª¨ë¸ì´ ìƒì„±í•œ ì½˜í…ì¸ ëŠ” response.output ì†ì„±ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë©°, ì•„ë˜ ì˜ˆì‹œëŠ” ì¶œë ¥(response) í˜•íƒœë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{\n",
        "  \"id\": \"resp_07354f378005a78400690d921c77408191b65a4eae80d15ac3\",\n",
        "  \"created_at\": 1762497052.0,\n",
        "  \"error\": null,\n",
        "  \"incomplete_details\": null,\n",
        "  \"instructions\": \"You are a helpful assistant. Reply in Korean.\",\n",
        "  \"metadata\": {},\n",
        "  \"model\": \"gpt-5-nano-2025-08-07\",\n",
        "  \"object\": \"response\",\n",
        "  \"output\": [\n",
        "    {\n",
        "      \"id\": \"rs_07354f378005a78400690d921d59b48191baa7723fcf20871b\",\n",
        "      \"summary\": [],\n",
        "      \"type\": \"reasoning\",\n",
        "      \"content\": null,\n",
        "      \"encrypted_content\": null,\n",
        "      \"status\": null\n",
        "    },\n",
        "    {\n",
        "      \"id\": \"msg_07354f378005a78400690d9223f82081918b05a37be09e6764\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"annotations\": [],\n",
        "          \"text\": \"ë°¤í•˜ëŠ˜ ì•„ë˜ ì‘ì€ ìˆ²ì—ì„œ í•œ ë§ˆë¦¬ì˜ ìœ ë‹ˆì½˜ì´ ë¶€ë“œëŸ¬ìš´ ë¹›ì„ ë‚´ë¿œìœ¼ë©° ì•„ì´ì˜ ê¿ˆìœ¼ë¡œ ê¸¸ì„ ì´ëŒì—ˆê³ , ì•„ì´ëŠ” í¬ê·¼í•œ ì ì— ë“¤ì—ˆë‹¤.\",\n",
        "          \"type\": \"output_text\",\n",
        "          \"logprobs\": []\n",
        "        }\n",
        "      ],\n",
        "      \"role\": \"assistant\",\n",
        "      \"status\": \"completed\",\n",
        "      \"type\": \"message\"\n",
        "    }\n",
        "  ],\n",
        "  \"parallel_tool_calls\": true,\n",
        "  \"temperature\": 1.0,\n",
        "  \"tool_choice\": \"auto\",\n",
        "  \"tools\": [],\n",
        "  \"top_p\": 1.0,\n",
        "  \"background\": false,\n",
        "  \"conversation\": null,\n",
        "  \"max_output_tokens\": null,\n",
        "  \"max_tool_calls\": null,\n",
        "  \"previous_response_id\": null,\n",
        "  \"prompt\": null,\n",
        "  \"prompt_cache_key\": null,\n",
        "  \"reasoning\": {\n",
        "    \"effort\": \"medium\",\n",
        "    \"generate_summary\": null,\n",
        "    \"summary\": null\n",
        "  },\n",
        "  \"safety_identifier\": null,\n",
        "  \"service_tier\": \"default\",\n",
        "  \"status\": \"completed\",\n",
        "  \"text\": {\n",
        "    \"format\": {\n",
        "      \"type\": \"text\"\n",
        "    },\n",
        "    \"verbosity\": \"medium\"\n",
        "  },\n",
        "  \"top_logprobs\": 0,\n",
        "  \"truncation\": \"disabled\",\n",
        "  \"usage\": {\n",
        "    \"input_tokens\": 31,\n",
        "    \"input_tokens_details\": {\n",
        "      \"cached_tokens\": 0\n",
        "    },\n",
        "    \"output_tokens\": 950,\n",
        "    \"output_tokens_details\": {\n",
        "      \"reasoning_tokens\": 896\n",
        "    },\n",
        "    \"total_tokens\": 981\n",
        "  },\n",
        "  \"user\": null,\n",
        "  \"billing\": {\n",
        "    \"payer\": \"developer\"\n",
        "  },\n",
        "  \"prompt_cache_retention\": null,\n",
        "  \"store\": true\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë‚´ê°€ ê·¸ëƒ¥ í•œë²ˆ í•´ë³´ë ¤ê³  ë„£ì€ê±°\n",
        "output = response.output\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ëª¨ë¸ì´ ìƒì„±í•œ ì½˜í…ì¸  ë°°ì—´ì€ `response.output` ì†ì„±ì— ë‹´ê¸°ë©°, ì•„ë˜ì™€ ê°™ì€ í˜•íƒœì…ë‹ˆë‹¤.\n",
        "\n",
        "ì´ ë°°ì—´ì—ëŠ” ë„êµ¬ í˜¸ì¶œ, ì¶”ë¡  ëª¨ë¸ì´ ë§Œë“  reasoning í† í° ë°ì´í„° ë“± ì—¬ëŸ¬ í•­ëª©ì´ í•¨ê»˜ ë“¤ì–´ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"output\": [\n",
        "    {\n",
        "      \"id\": \"rs_07354f378005a78400690d921d59b48191baa7723fcf20871b\",\n",
        "      \"summary\": [],\n",
        "      \"type\": \"reasoning\",\n",
        "      \"content\": null,\n",
        "      \"encrypted_content\": null,\n",
        "      \"status\": null\n",
        "    },\n",
        "    {\n",
        "      \"id\": \"msg_07354f378005a78400690d9223f82081918b05a37be09e6764\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"annotations\": [],\n",
        "          \"text\": \"ë°¤í•˜ëŠ˜ ì•„ë˜ ì‘ì€ ìˆ²ì—ì„œ í•œ ë§ˆë¦¬ì˜ ìœ ë‹ˆì½˜ì´ ë¶€ë“œëŸ¬ìš´ ë¹›ì„ ë‚´ë¿œìœ¼ë©° ì•„ì´ì˜ ê¿ˆìœ¼ë¡œ ê¸¸ì„ ì´ëŒì—ˆê³ , ì•„ì´ëŠ” í¬ê·¼í•œ ì ì— ë“¤ì—ˆë‹¤.\",\n",
        "          \"type\": \"output_text\",\n",
        "          \"logprobs\": []\n",
        "        }\n",
        "      ],\n",
        "      \"role\": \"assistant\",\n",
        "      \"status\": \"completed\",\n",
        "      \"type\": \"message\"\n",
        "    }\n",
        "  ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) ëª¨ë¸ ì„ íƒ ë° ì£¼ìš” íŒŒë¼ë¯¸í„°\n",
        "\n",
        "### model\n",
        "- ì‘ë‹µì„ ìƒì„±í•  ëª¨ë¸ ID(`gpt-5`, `o3` ë“±)ì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
        "- ëª¨ë¸ë³„ ì„±ëŠ¥ê³¼ ë¹„ìš©ì€ [OpenAI ëª¨ë¸ ê°€ì´ë“œ](https://platform.openai.com/docs/models)ì—ì„œ ìµœì‹  ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.\n",
        "\n",
        "### instructions\n",
        "- ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ì— ë„£ëŠ” ì‹œìŠ¤í…œ/ê°œë°œì ì§€ì¹¨ìœ¼ë¡œ, ì‘ë‹µì˜ í†¤Â·ì—­í• Â·ì œì•½ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "- `previous_response_id`ì™€ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ì´ì „ í„´ instructionsë¥¼ ëŠê³  ìƒˆ instructionìœ¼ë¡œ ì‰½ê²Œ êµì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    - `previous_response_id`: ì´ì „ ì‘ë‹µì˜ ê³ ìœ  IDì…ë‹ˆë‹¤. ì´ ê°’ì„ í™œìš©í•˜ë©´ ë©€í‹°í„´ ëŒ€í™”ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### input\n",
        "- í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, íŒŒì¼ ë“± ëª¨ë¸ì´ ë‹µë³€ì„ ìƒì„±í•  ë•Œ ì°¸ê³ í•˜ëŠ” ì…ë ¥ê°’ì…ë‹ˆë‹¤.\n",
        "\n",
        "### max_output_tokens\n",
        "- ê°€ì‹œ ì¶œë ¥ í† í°(visible output token)ê³¼ ì¶”ë¡  í† í°(resoning token)ì„ ëª¨ë‘ í¬í•¨í•œ ìµœëŒ€ ìƒì„± ê¸¸ì´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§\n",
        "í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì€ ëª¨ë¸ì— íš¨ê³¼ì ì¸ ì§€ì‹œë¬¸ì„ ì‘ì„±í•˜ì—¬ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ê²°ê³¼ë¥¼ ì¼ê´€ë˜ê²Œ ìƒì„±í•˜ë„ë¡ í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\n",
        "ëª¨ë¸ì´ ìƒì„±í•˜ëŠ” ê²°ê³¼ëŠ” ë¹„ê²°ì •ì ì´ë¯€ë¡œ, ì›í•˜ëŠ” ì¶œë ¥ì„ ì–»ê¸° ìœ„í•œ í”„ë¡¬í”„íŒ…ì€ ì˜ˆìˆ ê³¼ ê³¼í•™ì˜ ì¡°í•©ì…ë‹ˆë‹¤. ê·¸ëŸ¼ì—ë„ ê¸°ìˆ ê³¼ ëª¨ë²” ì‚¬ë¡€ë¥¼ ì ìš©í•˜ë©´ ì¼ê´€ë˜ê²Œ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "ë©”ì‹œì§€ ì—­í•  ì‚¬ìš©ê³¼ ê°™ì€ ì¼ë¶€ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ë²•ì€ ëª¨ë“  ëª¨ë¸ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ëª¨ë¸ë§ˆë‹¤ ìµœìƒì˜ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ ë‹¤ë¥¸ í”„ë¡¬í”„íŒ…ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°™ì€ ê³„ì—´ ëª¨ë¸ì˜ ì„œë¡œ ë‹¤ë¥¸ ìŠ¤ëƒ…ìƒ·ë„ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë” ë³µì¡í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•  ë•ŒëŠ” ë‹¤ìŒì„ ê¶Œì¥í•©ë‹ˆë‹¤:\n",
        "í”„ë¡œë•ì…˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ íŠ¹ì • ëª¨ë¸ ìŠ¤ëƒ…ìƒ·(ì˜ˆ: gpt-5-2025-08-07)ì— ê³ ì •í•˜ì—¬ ì¼ê´€ëœ ë™ì‘ ë³´ì¥\n",
        "í”„ë¡¬í”„íŠ¸ì˜ ë™ì‘ì„ ì¸¡ì •í•˜ëŠ” í‰ê°€ë¥¼ êµ¬ì¶•í•˜ì—¬ ë°˜ë³µí•˜ê±°ë‚˜ ëª¨ë¸ ë²„ì „ ë³€ê²½/ì—…ê·¸ë ˆì´ë“œ ì‹œ í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§\n",
        "í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë„êµ¬ì™€ ê¸°ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.responses.create(\n",
        "        model=\"gpt-5-mini\",\n",
        "        instructions=\"You are a helpful assistant. Reply in Korean.\", \n",
        "        input=\"Write a one-sentence bedtime story about a unicorn.\",\n",
        "    )\n",
        "\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `instructions` íŒŒë¼ë¯¸í„°ëŠ” ì‘ë‹µ ìƒì„± ì‹œ ìš°ì„  ì ìš©ë˜ëŠ” ì „ì—­ ì§€ì‹œë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
        "- `instructions`ëŠ” í†¤, ëª©í‘œ, ì˜¬ë°”ë¥¸ ì‘ë‹µ ì˜ˆì‹œ ë“± í–‰ë™ ì§€ì¹¨ì„ ì •ì˜í•˜ë©° `input` íŒŒë¼ë¯¸í„°ë³´ë‹¤ ë†’ì€ ìš°ì„ ìˆœìœ„ë¥¼ ê°€ì§‘ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> instruction ìˆ˜ì •: í•´ì ì²˜ëŸ¼ ë§í•´ì¤˜.\\\n",
        "instruction ìˆ˜ì •: ì¼ë³¸ì–´ë¡œ ë‹µí•´ì¤˜.\\\n",
        "input ìˆ˜ì •: write a three-sentence bedtime story about a rabbit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# max_output_tokens ì˜ˆì œ\n",
        "for tokens in [300, 500, 1000]:\n",
        "    response = client.responses.create(\n",
        "        model=\"gpt-5-mini\",\n",
        "        instructions=\"You are a helpful assistant. Reply in Korean.\",\n",
        "        input=\"Write a one-sentence bedtime story about a unicorn.\",\n",
        "        max_output_tokens=tokens\n",
        "    )\n",
        "    print(f\"\\n--- max_output_tokens={tokens} ---\")\n",
        "    print(response.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GPT-5 ëª¨ë¸ ë¼ì¸ì—…"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GPT-5 ì‹œë¦¬ì¦ˆëŠ” ì´ ì„¸ ê°€ì§€ ëª¨ë¸ì´ ìˆìŠµë‹ˆë‹¤. ì„ íƒ ì‹œ ì•„ë˜ íŠ¹ì„±ì„ ì°¸ê³ í•˜ì„¸ìš”.\n",
        "\n",
        "- gpt-5: ë³µì¡í•œ ì¶”ë¡ , ê´‘ë²”ìœ„í•œ ìƒì‹, ì½”ë“œ ì‘ì„± ë˜ëŠ” ë‹¤ë‹¨ê³„ ì‘ì—… ë“± ê°€ì¥ ì–´ë ¤ìš´ ë¬¸ì œì— ì í•©\n",
        "- gpt-5-mini: í•©ë¦¬ì ì¸ ë¹„ìš©ì— ê· í˜• ì¡íŒ ì„±ëŠ¥ê³¼ ì†ë„ë¥¼ ì œê³µí•˜ë©°, ì±„íŒ…ì´ë‚˜ ì¼ë°˜ì  ê³¼ì œì— ì¶”ì²œ\n",
        "- gpt-5-nano: ì•„ì£¼ ë¹ ë¥¸ ì²˜ë¦¬ì™€ ì €ë ´í•œ ë¹„ìš©ì´ ì¥ì ì´ë©°, ë‹¨ìˆœ ë¶„ë¥˜Â·ì§€ì‹œ ìˆ˜í–‰ ë“± ëŒ€ëŸ‰ ì²˜ë¦¬ ìš©ë„ì— ì í•©\n",
        "\n",
        "ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë¸ì´ ì‘ì•„ì§ˆìˆ˜ë¡ ë¹„ìš©ê³¼ ëŒ€ê¸°ì‹œê°„ì€ ë‚®ì•„ì§€ì§€ë§Œ, ë²”ìš© ìƒì‹ì´ë‚˜ ì°½ì˜ì„±ì´ ë‹¤ì†Œ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "ëª¨ë“  ì‘ì—…ì— í° ëª¨ë¸ì´ í•„ìš”í•œ ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ì˜ ì •ì˜ëœ ê³¼ì œë‚˜ êµ¬ì¡°í™”ëœ ìš”ì²­ì˜ ê²½ìš°, ì‘ì€ ëª¨ë¸ì´ ì˜¤íˆë ¤ ë” íš¨ìœ¨ì ì´ê³  ê²½ì œì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ê·¸ ì™¸ ë” ë§ì€ ëª¨ë¸ì€ ì•„ë˜ ë§í¬ì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”.\n",
        "https://platform.openai.com/docs/models\n",
        "\n",
        "> **ì£¼ì˜**: GPT-5 ê³„ì—´ì€ `temperature`, `top_p`, `logprobs` íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëŒ€ì‹  `reasoning.effort`, `text.verbosity`, `max_output_tokens`ë¡œ ì œì–´í•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) ëª¨ë¸ë³„ ì¶œë ¥ ì°¨ì´ ë¹„êµ ì‹¤ìŠµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "llm_prompt = \"ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë¬´ì—‡ì¸ì§€ ì„¤ëª…í•˜ì„¸ìš”.\"\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-5-nano\", # gpt-5, gpt-5-mini, gpt-5-nano\n",
        "    instructions=\"You are a helpful instructor. Reply in Korean and keep the answer well-structured.\",\n",
        "    input=llm_prompt,\n",
        ")\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> GPT-5 ì´ì™¸ì—ë„ ì˜›ë‚  ëª¨ë¸ë“¤ë„ í•œë²ˆ ëŒë ¤ë³´ë„ë¡ í•˜ê¸°, 4oë‚˜ 3.5ëŠ” ì–´ë–¤ ì–˜ê¸°ë¥¼ í•˜ëŠ”ì§€ í•œë²ˆ í•´ë³´ëŠ”ê±°ì§€ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ë‚´ê°€ ë³¼ë ¤ê³  ë„£ì€ ê°œë…\n",
        "\n",
        "âœ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ íë¦„ ì •ë¦¬\n",
        " \n",
        "GPT-5 APIì—ì„œ ìŠ¤íŠ¸ë¦¼ ì‘ë‹µì„ ë°›ì„ ë•Œ ë„ì°©í•˜ëŠ” ì´ë²¤íŠ¸ì™€ ì²˜ë¦¬ ìˆœì„œ ì˜ˆì‹œì…ë‹ˆë‹¤.\n",
        "| ë‹¨ê³„ | event.type                    | ì„¤ëª…                             | ì˜ˆì‹œ ì¶œë ¥                        |\n",
        "|------|-------------------------------|----------------------------------|----------------------------------|\n",
        "| 1    | response.output_text.delta    | ëª¨ë¸ì´ ìƒì„±í•œ í…ìŠ¤íŠ¸ ì¼ë¶€ ë„ì°©    | \"ëŒ€í˜•\", \" ì–¸ì–´\", \" ëª¨ë¸ì€\" ...   |\n",
        "| 2    | response.refusal.delta        | (ì„ íƒì ) ëª¨ë¸ì´ ê±°ì ˆ ë©”ì‹œì§€ ì „ì†¡  | \"ì£„ì†¡í•©ë‹ˆë‹¤, í•´ë‹¹ ìš”ì²­ì€...\"     |\n",
        "| 3    | response.error                | ì˜¤ë¥˜ ë°œìƒ                        | \"Rate limit exceeded\"            |\n",
        "| 4    | response.completed            | ëª¨ë“  í† í° ìƒì„± ì¢…ë£Œ ì•Œë¦¼          | \"Completed\"                      |\n",
        "| 5    | stream.get_final_response()   | ì „ì²´ ì‘ë‹µ ê²°ê³¼(ìµœì¢… í…ìŠ¤íŠ¸ ë“±)    | (ìµœì¢… ìƒì„± ë¬¸ì¥ ì „ì²´ ë°˜í™˜)       |\n",
        "- ë³´í†µ ë‹¨ê³„ 1ì—ì„œ ì—¬ëŸ¬ ë²ˆ eventê°€ ë“¤ì–´ì˜¤ë©°, ê° í† í°ì´ë‚˜ ì¡°ê°ë§ˆë‹¤ ë„ì°©í•©ë‹ˆë‹¤.\n",
        "- ë‹¨ê³„ 5ëŠ” ëª¨ë“  ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ í›„ í•œ ë²ˆë§Œ í˜¸ì¶œí•˜ì—¬, í†µí•© ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "- ê±°ì ˆ(event.type = response.refusal.delta) ë˜ëŠ” ì˜¤ë¥˜(event.type = response.error) ë°œìƒ ì‹œì—ëŠ” ì´í›„ ìƒì„±ì´ ì¤‘ë‹¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "- ì‹¤ ì„œë¹„ìŠ¤ì—ì„œëŠ” ê° event.typeì— ë”°ë¼ ì¶œë ¥ ì²˜ë¦¬, ì˜ˆì™¸ì²˜ë¦¬ë¥¼ ë¶„ê¸°í•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# ì´ê±´ ì¢€ í•˜ë©´ì„œ, ë„ˆë¬´ ì˜¤ë˜ê±¸ë¦¬ë‹ˆ, stream ê¸°ëŠ¥ì„ í•œë²ˆ í•´ë³´ê² ë‹¤ëŠ” ì‹ìœ¼ë¡œ ì „ê°œí•˜ë©´ ì¢‹ê² ë‹¤.\n",
        "# ìŠ¤íŠ¸ë¦¼ì„ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ë³´ê³  ì‹¶ìœ¼ì‹œë©´, ë…ìŠ¤ë¥¼ ë³´ì‹œë¼\n",
        "llm_prompt = \"ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë¬´ì—‡ì¸ì§€ ì„¤ëª…í•˜ì„¸ìš”.\"\n",
        "resp = client.responses.create(\n",
        "    model=\"gpt-5-nano\", # gpt-5, gpt-5-mini, gpt-5-nano\n",
        "    instructions=\"You are a helpful instructor. Reply in Korean and keep the answer well-structured.\",\n",
        "    input=llm_prompt,\n",
        "    stream = True,\n",
        ")\n",
        "\n",
        "for event in resp:\n",
        "    print(event)\n",
        "\n",
        "# ìœ„ì™€ ê°™ì´ í•˜ë©´ ì•ˆë˜ê³ ..\n",
        "\n",
        "for event in resp:\n",
        "    if event.type == \"response.output_text.delta\":\n",
        "        print(event.delta, end=\"\")\n",
        "        \n",
        "\n",
        "#print(resp.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) reasoning.effort Â· text.verbosity Â· max_output_tokens ì œì–´\n",
        "\n",
        "### reasoning.effort íŒŒë¼ë¯¸í„°ëŠ” ëª¨ë¸ì´ ì‘ë‹µì„ ìƒì„±í•˜ê¸° ì „ ì–¼ë§ˆë‚˜ ë§ì€ ì¶”ë¡  í† í°ì„ ì‚¬ìš©í• ì§€ ê²°ì •í•©ë‹ˆë‹¤.\n",
        "- reasoning effortëŠ” minimal, low, medium, highë¥¼ ì§€ì›í•©ë‹ˆë‹¤.\n",
        "   - minimal: ë¹ ë¥´ê³ , ìµœëŒ€í•œ ê°„ê²°í•˜ê²Œ ë‹µë³€\n",
        "   - low: ë¹ ë¥¸ ì†ë„, í† í° ì ê²Œ ì‚¬ìš©\n",
        "   - medium: ì†ë„-ì •í™•ì„± ê· í˜•, GPT-5ì˜ ê¸°ë³¸ê°’\n",
        "   - high: ê¼¼ê¼¼í•˜ê³ , ê¹Šì´ ìˆëŠ” ì„¤ëª… ì œê³µ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    input=\"ë‚¨ì‚°íƒ€ì›Œ ì™¸ë²½ì„ 1mm ë‘ê»˜ì˜ ê¸ˆìœ¼ë¡œ ë„ê¸ˆí•˜ë ¤ë©´ ê¸ˆì´ ëª‡ kgì´ë‚˜ í•„ìš”í• ê¹Œìš”?\",\n",
        "    reasoning={\n",
        "        \"effort\": \"minimal\" # low, medium, high\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### text.verbosity íŒŒë¼ë¯¸í„°ëŠ” ì¶œë ¥ ë‹µë³€ì˜ ê¸¸ì´ (í† í° ìˆ˜)ì— ì˜í–¥ì„ ì£¼ëŠ” ì„¤ì •ì…ë‹ˆë‹¤.\n",
        "- verbosityëŠ” low, medium, high ì˜µì…˜ì„ ì§€ì›í•©ë‹ˆë‹¤.\n",
        "   - low: ì§§ê³ , ê°„ê²°í•˜ë©° ìš”ì ë§Œ ì „ë‹¬ (ì˜ˆ: ìš”ì•½, ì½”ë“œ ìŠ¤ë‹ˆí«)\n",
        "   - medium: ì ì ˆí•œ ìƒì„¸ ì„¤ëª…ê³¼ ê¸¸ì´, GPT-5ì˜ ê¸°ë³¸ê°’\n",
        "   - high: ê¸¸ê³ , ìì„¸í•˜ë©° ì„¸ë¶€ ì„¤ëª…ì´ ë§ì€ ë‹µë³€ ì œê³µ\n",
        "- ì‘ìš© ëª©ì ì— ë”°ë¼ ìƒì„¸í•œ ì„¤ëª…ì´ í•„ìš”í•˜ë©´ high, ì•„ì£¼ ì§§ì€ ê²°ê³¼ê°€ ìš”êµ¬ë˜ë©´ lowë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "- ì‹¤ì œ ë‹µë³€ ê¸¸ì´ëŠ” ì…ë ¥ í”„ë¡¬í”„íŠ¸, max_output_tokens ë“± ë‹¤ë¥¸ ì„¤ì •ì—ë„ ì˜í–¥ë°›ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    input=\"ì‚¶, ìš°ì£¼ ê·¸ë¦¬ê³  ëª¨ë“  ê²ƒì— ëŒ€í•œ ê¶ê·¹ì ì¸ ì§ˆë¬¸ì˜ ë‹µì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
        "    text={\n",
        "        \"verbosity\": \"low\" # medium, high\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ì‹¤ìŠµ ì§€ì‹œ: ì›í•˜ëŠ” íŒŒë¼ë¯¸í„° ì¡°í•©ë§Œ ë‚¨ê¸°ê³  ì‹¶ë‹¤ë©´ ë£¨í”„ë¥¼ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì„ íƒì ìœ¼ë¡œ ì‹¤í–‰í•˜ë„ë¡ ìˆ˜ì •í•´ ë³´ì„¸ìš”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ë©”ì‹œì§€ ì—­í• (role)\n",
        "| ì—­í• (role)     | ì„¤ëª…                                                    | ìš°ì„ ìˆœìœ„                                 |\n",
        "| -------------- | ------------------------------------------------------- | ---------------------------------------- |\n",
        "| developer      | ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œìê°€ ì œê³µí•˜ëŠ” instructions             | ê°€ì¥ ë†’ìŒ (user ë©”ì‹œì§€ë³´ë‹¤ ìš°ì„ )         |\n",
        "| user           | ì‚¬ìš©ìê°€ ì œê³µí•˜ëŠ” instructions                        | developer ë©”ì‹œì§€ë³´ë‹¤ ë‚®ê³  assistantë³´ë‹¤ ë†’ìŒ |\n",
        "| assistant      | ëª¨ë¸ì´ ìƒì„±í•˜ëŠ” ì‘ë‹µ ë©”ì‹œì§€                              | ê°€ì¥ ë‚®ìŒ                                |\n",
        "\n",
        "- OpenAI ëª¨ë¸ ìŠ¤í™ì—ì„œëŠ” ì—­í•  ìš°ì„ ìˆœìœ„ë¥¼ `developer` > `user` > `assistant` ìˆœìœ¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ë©”ì‹œì§€ ì—­í• ì„ ì‚¬ìš©í•œ ë™ì¼ ì˜ˆì‹œ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ’¡ ì™œ inputì„ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì‚¬ìš©í• ê¹Œìš”?\n",
        "\n",
        "ìœ„ ì˜ˆì‹œì—ì„œ `input`ì„ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì‚¬ìš©í•œ ì´ìœ ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤:\n",
        "\n",
        "1. **ì—­í• (role) êµ¬ë¶„ì˜ í•„ìš”ì„±**\n",
        "   - `role: \"developer\"`: ì‹œìŠ¤í…œ ì§€ì‹œ (í•­ìƒ ìµœìš°ì„ )\n",
        "   - `role: \"user\"`: ì‚¬ìš©ì ì…ë ¥\n",
        "   - `role: \"assistant\"`: ëª¨ë¸ì˜ ì‘ë‹µ (ë©€í‹°í„´ ëŒ€í™”ì—ì„œ ì‚¬ìš©)\n",
        "\n",
        "2. **ë©€í‹°í„´ ëŒ€í™”ì˜ í•„ìˆ˜ êµ¬ì¡°**\n",
        "   - ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ìœ ì§€í•˜ë ¤ë©´ ì´ì „ ë©”ì‹œì§€ë“¤ì„ ëª¨ë‘ í¬í•¨í•´ì•¼ í•¨\n",
        "   - ê° ë©”ì‹œì§€ì˜ ì—­í• ì„ ëª…ì‹œí•´ì•¼ ëª¨ë¸ì´ ì˜¬ë°”ë¥´ê²Œ í•´ì„í•¨\n",
        "   - ì˜ˆ: `[developer ë©”ì‹œì§€, user ì§ˆë¬¸1, assistant ë‹µë³€1, user ì§ˆë¬¸2]` í˜•íƒœ\n",
        "\n",
        "3. **instructions íŒŒë¼ë¯¸í„°ì™€ì˜ ê´€ê³„**\n",
        "   - `instructions` íŒŒë¼ë¯¸í„°ëŠ” ë‹¨ì¼ ìš”ì²­ì— ì í•©\n",
        "   - ë©€í‹°í„´ ëŒ€í™”ì—ì„œëŠ” `input` ë¦¬ìŠ¤íŠ¸ ë‚´ `role: \"developer\"`ê°€ ë” ìœ ì—°í•¨\n",
        "   - ë‘˜ ë‹¤ ê°™ì€ ëª©ì (ì‹œìŠ¤í…œ ì§€ì‹œ)ì´ì§€ë§Œ, ì‚¬ìš© ìƒí™©ì— ë”°ë¼ ì„ íƒ\n",
        "\n",
        "**í•µì‹¬**: ë‹¨ìˆœí•œ ì§ˆë¬¸ì€ `instructions` + ë¬¸ìì—´ `input`ìœ¼ë¡œ ì¶©ë¶„í•˜ì§€ë§Œ, ëŒ€í™”ë¥¼ ì´ì–´ê°€ë ¤ë©´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ `input`ì´ ë” ì í•©í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    reasoning={\"effort\": \"low\"},\n",
        "    input=[\n",
        "        {\n",
        "            \"role\": \"developer\",\n",
        "            \"content\": \"í•´ì ì²˜ëŸ¼ ë§í•´ ì£¼ì„¸ìš”.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"í•œêµ­ì˜ ìˆ˜ë„ëŠ” ë¶€ì‚°ì¸ê°€ìš”?\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    input=[\n",
        "        {\"role\": \"user\", \"content\": \"knock knock.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
        "        {\"role\": \"user\", \"content\": \"Orange.\"},\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ê°•ì˜ì—ì„œ ìœ„ ë‚´ìš© ì„¤ëª…í•  ë•Œ, ë§ì”€ë“œë¦¬ë©´ ë  ê²ƒ ê°™ë‹¤. \n",
        "\n",
        "By using alternating user and assistant messages, you capture the previous state of a conversation in one request to the model.\n",
        "\n",
        "To manually share context across generated responses, include the model's previous response output as input, and append that input to your next request.\n",
        "\n",
        "In the following example, we ask the model to tell a joke, followed by a request for another joke. Appending previous responses to new requests in this way helps ensure conversations feel natural and retain the context of previous interactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) ë©€í‹°í„´ ëŒ€í™” êµ¬ì„± ì‹¤ìŠµ\n",
        "\n",
        "### ë©€í‹°í„´ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ê¸°ë³¸ ê°œë…\n",
        "- Responses APIëŠ” ìë™ìœ¼ë¡œ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë³´ì¡´í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì§ì ‘ ë©”ì‹œì§€ë¥¼ ëˆ„ì í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "- ê° í„´ë§ˆë‹¤ `role`ê³¼ `content`ê°€ ìˆëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ì¶”ê°€í•˜ê³ , ëª¨ë¸ì´ ìƒì„±í•œ ì‘ë‹µë„ íˆìŠ¤í† ë¦¬ì— í¬í•¨í•´ì•¼ ë‹¤ìŒ ìš”ì²­ì—ì„œ ì°¸ì¡°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "- `previous_response_id`ë¥¼ ì‚¬ìš©í•˜ë©´ ì´ì „ ì‘ë‹µì˜ ì…ë ¥ê³¼ ì¶œë ¥ì„ ê°„ë‹¨íˆ ì¬ì‚¬ìš©í•´ ì²´ì¸ ë˜ëŠ” ë¶„ê¸°í˜• ê¸°ë¡ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.responses.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    instructions=\"You are a helpful assistant. You must answer in Korean.\",\n",
        "    input=\"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\",\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask(question):\n",
        "    response = client.responses.create(\n",
        "        model=\"gpt-5-mini\",\n",
        "        instructions=\"You are a helpful assistant. You must answer in Korean.\",\n",
        "        input=question,\n",
        "    )\n",
        "    return response.output_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì²« ë²ˆì§¸ ì§ˆë¬¸\n",
        "ask(\"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë‘ ë²ˆì§¸ ì§ˆë¬¸\n",
        "ask(\"ë°©ê¸ˆ ë‹µë³€ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ ì£¼ì„¸ìš”\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ìœ„ì˜ ì˜ˆì‹œì²˜ëŸ¼ APIë¥¼ ë§¤ë²ˆ ë‹¨ì¼ í˜¸ì¶œë¡œë§Œ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì´ **ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ì§€ ëª»í•´ ì—‰ëš±í•œ ë‹µë³€**ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
        "\n",
        "í•´ê²° ë°©ë²•ì€ ê°„ë‹¨í•©ë‹ˆë‹¤. ê° í„´ì˜ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ëª¨ë¸ ì‘ë‹µì„ ìˆœì„œëŒ€ë¡œ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€í•´ ë‹¤ìŒ ìš”ì²­ì˜ `input`ìœ¼ë¡œ ì „ë‹¬í•˜ë©´, ì±—ë´‡ì´ ì´ì „ ë¬¸ë§¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì°¸ê³ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "context = [\n",
        "    {\n",
        "        \"role\": \"developer\",\n",
        "        \"content\": \"You are a helpful assistant. You must answer in Korean.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\",\n",
        "    },\n",
        "]\n",
        "\n",
        "res1 = client.responses.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    input=context,\n",
        ")\n",
        "\n",
        "first_answer = res1.output_text\n",
        "print(\"ì²« ë²ˆì§¸ ì‘ë‹µ:\", first_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "context.append({\"role\": \"assistant\", \"content\": first_answer})\n",
        "context.append([{\"role\": el.role, \"content\": el.content} for el in res1.output])\n",
        "context.append({\"role\": \"user\", \"content\": \"ë°©ê¸ˆ ë‹µë³€ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ ì£¼ì„¸ìš”.\"})\n",
        "\n",
        "res2 = client.responses.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    input=context,\n",
        ")\n",
        "\n",
        "print(\"\\në‘ ë²ˆì§¸ ì‘ë‹µ:\", res2.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ì´ì œ Responses API í˜¸ì¶œì„ í•¨ìˆ˜ë¡œ ê°ì‹¸ì„œ íˆìŠ¤í† ë¦¬ë¥¼ ìë™ìœ¼ë¡œ ê´€ë¦¬í•´ ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask(question, message_history=None, *, model=\"gpt-5-mini\", reasoning_effort=\"medium\", verbosity=\"medium\"):\n",
        "    if message_history is None:\n",
        "        message_history = [\n",
        "            {\n",
        "                \"role\": \"developer\",\n",
        "                \"content\": \"You are a helpful assistant. You must answer in Korean.\",\n",
        "            }\n",
        "        ]\n",
        "\n",
        "    message_history.append({\"role\": \"user\", \"content\": question})\n",
        "\n",
        "    response = client.responses.create(\n",
        "        model=model,\n",
        "        input=message_history,\n",
        "        reasoning={\"effort\": reasoning_effort},\n",
        "        text={\"verbosity\": verbosity},\n",
        "    )\n",
        "\n",
        "    message_history.append({\"role\": \"assistant\", \"content\": response.output_text})\n",
        "    return message_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ìµœì´ˆ ì§ˆë¬¸\n",
        "message_history = ask(\"ì–‘ìì—­í•™ì— ëŒ€í•´ì„œ ì‰½ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”\")\n",
        "# ìµœì´ˆ ë‹µë³€\n",
        "print(message_history[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "message_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë‘ ë²ˆì§¸ ì§ˆë¬¸\n",
        "message_history = ask(\n",
        "    \"ì´ì „ì˜ ë‚´ìš©ì„ ì˜ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”\", message_history=message_history\n",
        ")\n",
        "# ë‘ ë²ˆì§¸ ë‹µë³€\n",
        "print(message_history[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### previous_response_idë¥¼ í™œìš©í•œ ë©€í‹°í„´ ëŒ€í™” ì˜ˆì‹œ\n",
        "- The unique ID of the previous response to the model.\n",
        "- Use this to create multi-turn conversations. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "context = [\n",
        "    {\"role\": \"developer\", \"content\": \"You are a helpful assistant. You must answer in Korean.\"},\n",
        "    {\"role\": \"user\", \"content\": \"ì¬ë°ŒëŠ” ë†ë‹´ í•´ì¤˜.\"},\n",
        "]\n",
        "\n",
        "res1 = client.responses.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    input=context,\n",
        ")\n",
        "\n",
        "print(res1.output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì´ì „ ì‘ë‹µì˜ response_idë¥¼ í™œìš©í•˜ì—¬ ëŒ€í™” ì´ì–´ê°€ê¸°\n",
        "res2 = client.responses.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    input=[{\"role\": \"user\", \"content\": \"ê·¸ê²Œ ì™œ ì¬ë°ŒëŠ”ê±´ì§€ ì„¤ëª…í•´ì¤˜.\"}],\n",
        "    previous_response_id=res1.id,\n",
        ")\n",
        "\n",
        "print(res2.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another way to manage conversation state is to share context across generated responses with the previous_response_id parameter. This parameter lets you chain responses and create a threaded conversation.\n",
        "ëŒ€ì‰¬ë³´ë“œì— ê°€ì„œë„ ì‘ë‹µì— ëŒ€í•´ í™•ì¸í•´ë³¼ ìˆ˜ ìˆìŒ\n",
        "30ì¼ ê°„ ê¸°ë³¸ì ìœ¼ë¡œ ë³´ê´€ë˜ë©°, ë„ê³  ì‹¶ìœ¼ë©´, store=false ì˜µì…˜ì„ ë„£ì–´ì„œ ëŒ ìˆ˜ ìˆë‹¤"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ì°¸ê³  ìë£Œ\n",
        "- https://platform.openai.com/docs\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ajou_lecture",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
